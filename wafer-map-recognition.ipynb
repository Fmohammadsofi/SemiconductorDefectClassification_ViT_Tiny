{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-06T06:57:56.944523Z",
     "iopub.status.busy": "2025-02-06T06:57:56.943501Z",
     "iopub.status.idle": "2025-02-06T06:57:56.986976Z",
     "shell.execute_reply": "2025-02-06T06:57:56.985762Z",
     "shell.execute_reply.started": "2025-02-06T06:57:56.944482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:57:57.337584Z",
     "iopub.status.busy": "2025-02-06T06:57:57.337204Z",
     "iopub.status.idle": "2025-02-06T06:57:57.366522Z",
     "shell.execute_reply": "2025-02-06T06:57:57.365366Z",
     "shell.execute_reply.started": "2025-02-06T06:57:57.337550Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arr_0', 'arr_1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#data = np.load(\"/kaggle/input/mixedtype-wafer-defect-datasets/Wafer_Map_Datasets.npz\")\n",
    "data = np.load(\"Wafer_Map_Datasets.npz\")\n",
    "print(data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:57:57.785655Z",
     "iopub.status.busy": "2025-02-06T06:57:57.784505Z",
     "iopub.status.idle": "2025-02-06T06:58:01.302934Z",
     "shell.execute_reply": "2025-02-06T06:58:01.301761Z",
     "shell.execute_reply.started": "2025-02-06T06:57:57.785580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images = data[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:58:01.305490Z",
     "iopub.status.busy": "2025-02-06T06:58:01.304992Z",
     "iopub.status.idle": "2025-02-06T06:58:02.198160Z",
     "shell.execute_reply": "2025-02-06T06:58:02.197001Z",
     "shell.execute_reply.started": "2025-02-06T06:58:01.305447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 1 1 1 2 1 1 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1\n",
      " 2 2 1 1 1 1 2 1 1 1 1 1 1 2 1]\n",
      "AxesImage(size=(52, 52))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = data[\"arr_0\"]\n",
    "print(train[25][25]) # 单通道0-1-2三值图片\n",
    "print(plt.imshow(train[20010]))\n",
    "label = data[\"arr_1\"]\n",
    "#for i in range(50):\n",
    "#    print(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:58:21.015459Z",
     "iopub.status.busy": "2025-02-06T06:58:21.014662Z",
     "iopub.status.idle": "2025-02-06T06:58:21.045718Z",
     "shell.execute_reply": "2025-02-06T06:58:21.044521Z",
     "shell.execute_reply.started": "2025-02-06T06:58:21.015421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1  2  3  4  5  6  7\n",
      "1  0  1  0  0  0  1  0    2000\n",
      "0  0  0  0  0  0  0  0    1000\n",
      "1  0  0  0  1  0  0  0    1000\n",
      "0  1  1  0  0  0  0  0    1000\n",
      "                  1  0    1000\n",
      "            1  0  0  0    1000\n",
      "                  1  0    1000\n",
      "1  0  0  0  0  0  0  0    1000\n",
      "                  1  0    1000\n",
      "            1  0  1  0    1000\n",
      "0  1  0  1  1  0  0  0    1000\n",
      "1  0  0  1  0  0  0  0    1000\n",
      "                  1  0    1000\n",
      "            1  0  0  0    1000\n",
      "                  1  0    1000\n",
      "      1  0  0  0  0  0    1000\n",
      "            1  0  0  0    1000\n",
      "0  1  0  1  1  0  1  0    1000\n",
      "            0  0  1  0    1000\n",
      "   0  1  0  0  0  0  0    1000\n",
      "   1  0  1  0  0  0  0    1000\n",
      "   0  0  0  0  0  1  0    1000\n",
      "            1  0  0  0    1000\n",
      "                  1  0    1000\n",
      "         1  0  0  0  0    1000\n",
      "                  1  0    1000\n",
      "            1  0  0  0    1000\n",
      "                  1  0    1000\n",
      "1  0  1  0  1  0  1  0    1000\n",
      "0  0  1  0  0  0  1  0    1000\n",
      "            1  0  0  0    1000\n",
      "                  1  0    1000\n",
      "   1  0  0  0  0  0  0    1000\n",
      "                  1  0    1000\n",
      "            1  0  0  0    1000\n",
      "                  1  0    1000\n",
      "   0  0  0  0  0  0  1     866\n",
      "               1  0  0     149\n",
      "Name: count, dtype: int64\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(label).value_counts())\n",
    "print(pd.DataFrame(label).isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:58:21.368079Z",
     "iopub.status.busy": "2025-02-06T06:58:21.367639Z",
     "iopub.status.idle": "2025-02-06T06:58:21.373784Z",
     "shell.execute_reply": "2025-02-06T06:58:21.372597Z",
     "shell.execute_reply.started": "2025-02-06T06:58:21.368033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Distribution graphs (histogram/bar graph) of column data\n",
    "# def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "#     nunique = df.nunique()\n",
    "#     df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
    "#     nRow, nCol = df.shape\n",
    "#     columnNames = list(df)\n",
    "#     nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
    "#     plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "#     for i in range(min(nCol, nGraphShown)):\n",
    "#         plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "#         columnDf = df.iloc[:, i]\n",
    "#         if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
    "#             valueCounts = columnDf.value_counts()\n",
    "#             valueCounts.plot.bar()\n",
    "#         else:\n",
    "#             columnDf.hist()\n",
    "#         plt.ylabel('counts')\n",
    "#         plt.xticks(rotation = 90)\n",
    "#         plt.title(f'{columnNames[i]} (column {i})')\n",
    "#     plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
    "#     plt.show()\n",
    "\n",
    "# df = pd.DataFrame(train[0])\n",
    "# plotPerColumnDistribution(df, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:58:21.630378Z",
     "iopub.status.busy": "2025-02-06T06:58:21.629384Z",
     "iopub.status.idle": "2025-02-06T06:58:22.287415Z",
     "shell.execute_reply": "2025-02-06T06:58:22.286488Z",
     "shell.execute_reply.started": "2025-02-06T06:58:21.630338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, label, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:58:22.289707Z",
     "iopub.status.busy": "2025-02-06T06:58:22.289330Z",
     "iopub.status.idle": "2025-02-06T06:58:22.294142Z",
     "shell.execute_reply": "2025-02-06T06:58:22.293010Z",
     "shell.execute_reply.started": "2025-02-06T06:58:22.289659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X_train = X_train.astype(float)\n",
    "# X_test = X_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:58:22.295910Z",
     "iopub.status.busy": "2025-02-06T06:58:22.295502Z",
     "iopub.status.idle": "2025-02-06T06:58:22.518045Z",
     "shell.execute_reply": "2025-02-06T06:58:22.516981Z",
     "shell.execute_reply.started": "2025-02-06T06:58:22.295867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 1 0]\n",
      "(30412, 8)\n",
      "(7603, 8)\n",
      "(30412, 1, 52, 52)\n",
      "(30412, 52, 52)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGfCAYAAAD22G0fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgkElEQVR4nO3df2xV9f3H8dfFwhWwvdOp99JQTZWik18qbAWi0q9KTecMjn9UjGHbPypoqGxBkWVeFtYCJogGZMEZrVlY98dk+semdJmULYSkIESCzuCoWAO1ccF7Kz9agc/3D8edtfSWe+49933uvc9HcpLec84993M/596+8mnf53NCzjknAAAMjLBuAACgdBFCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADNlfh34xRdf1LPPPqujR49q0qRJWr9+vW699dZhn3f27FkdOXJE5eXlCoVCfjUPAOAT55x6e3tVWVmpESOGGes4H7S2trqRI0e6l156yb3//vtuyZIlbuzYse7w4cPDPrerq8tJYmFhYWEp8KWrq2vY3/kh53I/gWltba1uvvlmbdq0KbXue9/7nu699141NzenfW4ikdB3vvMd3aIfqkwjc900AIDPTusr/VN/0RdffKFIJJJ235z/Oa6/v1979uzRU089NWB9fX29du7cOWj/vr4+9fX1pR739vb+t2EjVRYihACg4Px3aHMh/1LJeWHC559/rjNnzigajQ5YH41G1d3dPWj/5uZmRSKR1FJVVZXrJgEAAsq36rhvJ6Bz7rypuHz5ciUSidTS1dXlV5MAAAGT8z/HXX755brooosGjXp6enoGjY4kKRwOKxwO57oZAIACkPMQGjVqlKZPn662tjb9+Mc/Tq1va2vTvHnzcv1ywAV5+8g+6yb47to/PjLktn/f91vPz53wxC7PbQKG48t1QkuXLtVDDz2kGTNmaNasWdq8ebM++eQTPfLI0B90AEDp8SWE7rvvPv3nP//Rr3/9ax09elSTJ0/WX/7yF1199dV+vBwAoED5NmPCokWLtGjRIr8ODwAoAswdBwAwQwgBAMwQQgAAM779TwjwinLq9OXUuX694V7zWuW+qpWyb5zDSAgAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmKFEG1kplHLqrEqUh3muF15fz4/y7eFe0w8fPTdzyG1e+4ay78LESAgAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmAk555x1I74pmUwqEomoTvNUFhpp3ZyS4FeZtddSY6/lwvkus/aLxfsolr7zivLu3DrtvtJ2vaFEIqGKioq0+zISAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlu5VAignbLhVK/FiidfN86wq/XLCTpbi3BNUT+YiQEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMxQol1ELG7J4FW+S639KlEulJLxILVFKpx+kyjf9hsjIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABgJuScc9aN+KZkMqlIJKI6zVNZaKR1c0z4UWodtLJXPxRS2W+xKJQ+D9os4sVe2n3afaXtekOJREIVFRVp92UkBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMMIu2Eb9mvC4UXkt7C6UkuFTku8+9nv+gfTaYmft/GAkBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOUaPso32XYQStD9YPX92gxizLl5LlXLP2W9rOhod9jMZZvMxICAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYyLtHesWOHnn32We3Zs0dHjx7V1q1bde+996a2O+e0cuVKbd68WceOHVNtba02btyoSZMm5bLdRa2QylALZVZjv16vUMqwLUrUMTT6+38yHgkdP35c06ZN04YNG867fe3atVq3bp02bNigjo4OxWIxzZ07V729vVk3FgBQXDIeCTU0NKihoeG825xzWr9+vVasWKH58+dLklpaWhSNRrVlyxY9/PDD2bUWAFBUcvo/oc7OTnV3d6u+vj61LhwOa86cOdq5c+d5n9PX16dkMjlgAQCUhpyGUHd3tyQpGo0OWB+NRlPbvq25uVmRSCS1VFVV5bJJAIAA86U6LhQKDXjsnBu07pzly5crkUiklq6uLj+aBAAIoJxOYBqLxSR9PSIaN25can1PT8+g0dE54XBY4XA4l80AABSInIZQdXW1YrGY2tradNNNN0mS+vv71d7erjVr1uTypQLD60zZxVKiWSizT/tVolwo57FQ2jmcQimJ98tHz80ccluhzrCdcQh9+eWX+uijj1KPOzs7tW/fPl122WW66qqr1NjYqKamJtXU1KimpkZNTU0aM2aMFixYkNOGAwAKX8YhtHv3bv3f//1f6vHSpUslSQsXLtSrr76qZcuW6eTJk1q0aFHqYtVt27apvLw8d60GABSFjEOorq5Ozrkht4dCIcXjccXj8WzaBQAoAcwdBwAwQwgBAMwQQgAAMyGX7h88BpLJpCKRiOo0T2WhkdbNkZT/Muzhyom9vF42pa1en1vq5bTplEKf+vE5xtCG6++7Km/MT0MknXZfabveUCKRUEVFRdp9GQkBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADM5nUW7UHktwfaLHyWqXst+h3uuH88rpPLlfJfT+8WPvgvae/TKj7ZafI6DOgM3IyEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCY4TohBW/q+Hxf72JxDU0h9XmQ+HXtTb7PRyGd/0Lpm6D16YViJAQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzIScc866Ed+UTCYViURUp3kqC43M2XGzuV2DH2WxhVpOaa1QSqktBK18O0hlz8Px4/Yh6QTt/afj5TYPp91X2q43lEgkVFFRkXZfRkIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAyzaMu/ckk/jltIJaH5LnstBUErw/Z6zHyXbw/XN4Uyc3U2/RbUknlGQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCXayq58M0hlqBa8lgVThu2NX/2W79L/IF2+UCq8/q6664kbfWjN/zASAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmiqpE++0j+zw9L5ty0XyXYQdthuEgzYYdpLaUCj9mSi+Wyx4Kqa3ppPu9elfljVkfn5EQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBTVCXapcCi7NOP8mY/yqkpw/Yu3+cjSLNvW/DaN35dTpJOutecoF1em5PCSAgAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmMmoRLu5uVmvv/66/vWvf2n06NGaPXu21qxZo+uuuy61j3NOK1eu1ObNm3Xs2DHV1tZq48aNmjRpUs4bHwTFUDI63Hvwo/SZcurcy6bMOt/nI9+XGmQzU7wf5dTpjllq342MRkLt7e1avHixdu3apba2Np0+fVr19fU6fvx4ap+1a9dq3bp12rBhgzo6OhSLxTR37lz19vbmvPEAgMKW0UjorbfeGvD4lVde0ZVXXqk9e/botttuk3NO69ev14oVKzR//nxJUktLi6LRqLZs2aKHH344dy0HABS8rP4nlEgkJEmXXXaZJKmzs1Pd3d2qr69P7RMOhzVnzhzt3LnzvMfo6+tTMpkcsAAASoPnEHLOaenSpbrllls0efJkSVJ3d7ckKRqNDtg3Go2mtn1bc3OzIpFIaqmqqvLaJABAgfEcQo899pjee+89/eEPfxi0LRQKDXjsnBu07pzly5crkUiklq6uLq9NAgAUGE8TmD7++ON68803tWPHDo0fPz61PhaLSfp6RDRu3LjU+p6enkGjo3PC4bDC4bCXZgAAClzIOecudGfnnB5//HFt3bpV27dvV01NzaDtlZWVeuKJJ7Rs2TJJUn9/v6688kqtWbPmggoTksmkIpGI6jRPZaGRg7a/fWTfhTb3ghVDmfVwSq3sE8XJj1LqbBTL9yrX/XP21Cl98tQvlUgkVFFRkXbfjEZCixcv1pYtW/TGG2+ovLw89X+eSCSi0aNHKxQKqbGxUU1NTaqpqVFNTY2ampo0ZswYLViwwPs7AgAUpYxCaNOmTZKkurq6AetfeeUV/eQnP5EkLVu2TCdPntSiRYtSF6tu27ZN5eXlOWkwAKB4ZBRCF/KXu1AopHg8rng87rVNAIASwdxxAAAzhBAAwAwhBAAwQwgBAMxkdJ1QPvh1nVDQrgXy45oGv66TKJZrIdIJ0vUn9Le3929xLZAf7yOdfL+eV8nes7p04qELuk6IkRAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMOPpfkJBFbTp2vNdFl4o5ZsWsjkXQSvvLwbFUvae7/Jti++xl/Nx9tQpSb+8oH0ZCQEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM8yifQH8KMMM0kzZxVK+Xeql1IV0HoN2OUHQ2lMohuq3s6dO6ZOnfsks2gCAYCOEAABmCCEAgBlCCABghhACAJghhAAAZphF2+i4XkutS70Mu9QVy3nM98z02fRbIc3qXYgYCQEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM0VVou3HbNfDPdeP58EbP2YYR/4FaYb54Y6L7DESAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmiqpE2yvKd4tDKZzHbC41KHZ+lWF77fNiOVd+f68YCQEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM5RoXwCv5ZSlUDIcJNmUvRbKuSqk0l6v/Pi+ZTNTvtf2FMu58nt2ekZCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMJdoXoFDKd70a7v0VS6lpoSiW/g7S7NPZfMa9fv+L5Tz6jZEQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADCTUYn2pk2btGnTJn388ceSpEmTJulXv/qVGhoaJEnOOa1cuVKbN2/WsWPHVFtbq40bN2rSpEk5b/j5+FVKbTGrr5djwjvKaXPPjz61+PwXy2fDj99HuZDRSGj8+PFavXq1du/erd27d+v222/XvHnzdODAAUnS2rVrtW7dOm3YsEEdHR2KxWKaO3euent7fWk8AKCwZRRC99xzj374wx9q4sSJmjhxon7zm9/okksu0a5du+Sc0/r167VixQrNnz9fkydPVktLi06cOKEtW7b41X4AQAHz/D+hM2fOqLW1VcePH9esWbPU2dmp7u5u1dfXp/YJh8OaM2eOdu7cOeRx+vr6lEwmBywAgNKQcQjt379fl1xyicLhsB555BFt3bpVN9xwg7q7uyVJ0Wh0wP7RaDS17Xyam5sViURSS1VVVaZNAgAUqIxD6LrrrtO+ffu0a9cuPfroo1q4cKHef//91PZQKDRgf+fcoHXftHz5ciUSidTS1dWVaZMAAAUq4wlMR40apQkTJkiSZsyYoY6ODj3//PN68sknJUnd3d0aN25cav+enp5Bo6NvCofDCofDmTYDAFAEsr5OyDmnvr4+VVdXKxaLqa2tLbWtv79f7e3tmj17drYvAwAoQhmNhJ5++mk1NDSoqqpKvb29am1t1fbt2/XWW28pFAqpsbFRTU1NqqmpUU1NjZqamjRmzBgtWLDAr/YPELRrb4I0PTzXFxU/i+tAgnrtybcN1xY/vh/Fcp2U36+ZUQh99tlneuihh3T06FFFIhFNnTpVb731lubOnStJWrZsmU6ePKlFixalLlbdtm2bysvLfWk8AKCwZRRCL7/8ctrtoVBI8Xhc8Xg8mzYBAEoEc8cBAMwQQgAAM4QQAMAMIQQAMBNyzjnrRnxTMplUJBJRneapLDQyo+e+fWSfp9cslunhvZbLZvP+g3SbC3gXpPNh8TnON4vPuB/9M+GJXeddf9p9pe16Q4lEQhUVFWmPwUgIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJihRFuUdpZ6iTr8UUjfqyDxWoZuMVM6JdoAgIJGCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMxkdHtv2PNa9pqufHO40s5CKbWlDNu7Ui9v92N27qAdM6jnkZEQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBTVLNop+N1hm3Je/kqpc25V+qlxH7x47MatBLlIMl33wzHy0zZ6TCLNgCgIBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM5RoZynfpaZBK1H1WhZKqXXu+dWnfnweg/S84eT7e+XHdyqb495VeWPGz6FEGwBQEAghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUq0sxSk8s2gzSLsR6l10N5jOoVUvl5I/TqUbL4bhTLjt8X3n1m0AQBFixACAJghhAAAZgghAIAZQggAYIYQAgCYKZkS7XQ+em5m2u35LosMWhl2ocx4XAxlxpLN+S/1Ps+3oM0U72Wm7HQo0QYAFARCCABghhACAJghhAAAZgghAIAZQggAYKbMugFBELRyyaDNsOv1uUGaYTwbpVBOHrT2eBG0SxvSCdp3fIIynyk7VxgJAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzWYVQc3OzQqGQGhsbU+ucc4rH46qsrNTo0aNVV1enAwcOZNtOAEAR8nydUEdHhzZv3qypU6cOWL927VqtW7dOr776qiZOnKhVq1Zp7ty5+vDDD1VeXp51g/0w3DTmbx/Z5+m4+b4FQtCuhfDKj+tyLF6zUK6vspDv70axyOpaoCfsrgVKx9NI6Msvv9SDDz6ol156SZdeemlqvXNO69ev14oVKzR//nxNnjxZLS0tOnHihLZs2ZKzRgMAioOnEFq8eLHuvvtu3XnnnQPWd3Z2qru7W/X19al14XBYc+bM0c6dO7NrKQCg6GT857jW1la9++676ujoGLStu7tbkhSNRgesj0ajOnz48HmP19fXp76+vtTjZDKZaZMAAAUqo5FQV1eXlixZot///ve6+OKLh9wvFAoNeOycG7TunObmZkUikdRSVVWVSZMAAAUsoxDas2ePenp6NH36dJWVlamsrEzt7e164YUXVFZWlhoBnRsRndPT0zNodHTO8uXLlUgkUktXV5fHtwIAKDQZ/Tnujjvu0P79+wes++lPf6rrr79eTz75pK655hrFYjG1tbXppptukiT19/ervb1da9asOe8xw+GwwuGwx+YDAApZRiFUXl6uyZMnD1g3duxYffe7302tb2xsVFNTk2pqalRTU6OmpiaNGTNGCxYsyF2r8yxdCfe/j5R2yWi++TEF/nDbLUrGi13QyteLofQ7qCXYw8n5/YSWLVumkydPatGiRTp27Jhqa2u1bdu2wF4jBACwk3UIbd++fcDjUCikeDyueDye7aEBAEWOueMAAGYIIQCAGUIIAGCGEAIAmAk555x1I74pmUwqEomoTvNUFhpp3ZyseJ19O98lodmUGRdK+epw8j1TtleF0k74o1DKsE+7r7RdbyiRSKiioiLtvoyEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZSrR95LVEOx2LUls/Zpj243349Xp+zJRdKKXdwwnS+c9GoXzGKdEGACCHCCEAgBlCCABghhACAJghhAAAZgghAICZrG/vjaHdVXnjkNs+em7mkNu8ltP6NcOy11JTr6WtfrQlG368j3wLWkl0Ohal3X58xr0qlDLsXGEkBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMMIt2AJX67Nt+GK6U2o+ZkgupDN2rQpl9Oht+nMdiL8NmFm0AQEEghAAAZgghAIAZQggAYIYQAgCYIYQAAGYo0S4i6Uq7/Sp7LZRS22xKtP2Q79m3g3QuhlNIZd/FXmrtFSXaAICCQAgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMlFk3ALlzV+WNQ27795H8l6/mu2Q232XPw71muvfox+zb6WRzTK/nyo/ZpynDLj6MhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGWzlAHz0305fjBuk6oUK6lUE6fl0LVSz94xXXAuUWt3IAABQEQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGWzkgq/LUdOXd+b5dQamz6G8/juu1XJwy68LESAgAYIYQAgCYIYQAAGYIIQCAGUIIAGAmoxCKx+MKhUIDllgsltrunFM8HldlZaVGjx6turo6HThwIOeNBgAUh4xLtCdNmqS//e1vqccXXXRR6ue1a9dq3bp1evXVVzVx4kStWrVKc+fO1Ycffqjy8vLctBiB4rUs9q4nbhxy29tH9g25LWizPXududvr84L2/v0op54gSq1LScZ/jisrK1MsFkstV1xxhaSvR0Hr16/XihUrNH/+fE2ePFktLS06ceKEtmzZkvOGAwAKX8YhdPDgQVVWVqq6ulr333+/Dh06JEnq7OxUd3e36uvrU/uGw2HNmTNHO3fuzF2LAQBFI6M/x9XW1uq1117TxIkT9dlnn2nVqlWaPXu2Dhw4oO7ubklSNBod8JxoNKrDhw8Pecy+vj719fWlHieTyUyaBAAoYBmFUENDQ+rnKVOmaNasWbr22mvV0tKimTO/nr4lFAoNeI5zbtC6b2pubtbKlSszaQYAoEhkVaI9duxYTZkyRQcPHkxVyZ0bEZ3T09MzaHT0TcuXL1cikUgtXV1d2TQJAFBAsgqhvr4+ffDBBxo3bpyqq6sVi8XU1taW2t7f36/29nbNnj17yGOEw2FVVFQMWAAApSGjP8f94he/0D333KOrrrpKPT09WrVqlZLJpBYuXKhQKKTGxkY1NTWppqZGNTU1ampq0pgxY7RgwQK/2o8idFfljUNu81q+m26272z4UYadjl+l3emOm+58ANnKKIQ+/fRTPfDAA/r88891xRVXaObMmdq1a5euvvpqSdKyZct08uRJLVq0SMeOHVNtba22bdvGNUIAgPPKKIRaW1vTbg+FQorH44rH49m0CQBQIpg7DgBghhACAJghhAAAZjKewNRvzjlJ0ml9JTnjxqBonD11Ku+vmew9O+Q2r+3x45jDHfe0+8rzcVGaTuvrz8y53+fphNyF7JVHn376qaqqqqybAQDIUldXl8aPH592n8CF0NmzZ3XkyBGVl5crFAopmUyqqqpKXV1dXMj6LfTN0OibodE3Q6NvhpZJ3zjn1Nvbq8rKSo0Ykf6/PoH7c9yIESPOm5zMpjA0+mZo9M3Q6Juh0TdDu9C+iUQiF3Q8ChMAAGYIIQCAmcCHUDgc1jPPPKNwOGzdlMChb4ZG3wyNvhkafTM0v/omcIUJAIDSEfiREACgeBFCAAAzhBAAwAwhBAAwE+gQevHFF1VdXa2LL75Y06dP1z/+8Q/rJpnYsWOH7rnnHlVWVioUCunPf/7zgO3OOcXjcVVWVmr06NGqq6vTgQMHbBqbR83Nzfr+97+v8vJyXXnllbr33nv14YcfDtinVPtm06ZNmjp1aurCwlmzZumvf/1ranup9sv5NDc3p+4MfU4p9088HlcoFBqwxGKx1PZc901gQ+iPf/yjGhsbtWLFCu3du1e33nqrGhoa9Mknn1g3Le+OHz+uadOmacOGDefdvnbtWq1bt04bNmxQR0eHYrGY5s6dq97e3jy3NL/a29u1ePFi7dq1S21tbTp9+rTq6+t1/Pjx1D6l2jfjx4/X6tWrtXv3bu3evVu333675s2bl/plUar98m0dHR3avHmzpk6dOmB9qffPpEmTdPTo0dSyf//+1Lac940LqB/84AfukUceGbDu+uuvd0899ZRRi4JBktu6dWvq8dmzZ10sFnOrV69OrTt16pSLRCLut7/9rUEL7fT09DhJrr293TlH33zbpZde6n73u9/RL//V29vrampqXFtbm5szZ45bsmSJc47PzTPPPOOmTZt23m1+9E0gR0L9/f3as2eP6uvrB6yvr6/Xzp07jVoVTJ2dneru7h7QV+FwWHPmzCm5vkokEpKkyy67TBJ9c86ZM2fU2tqq48ePa9asWfTLfy1evFh333237rzzzgHr6R/p4MGDqqysVHV1te6//34dOnRIkj99E7gJTCXp888/15kzZxSNRgesj0aj6u7uNmpVMJ3rj/P11eHDhy2aZMI5p6VLl+qWW27R5MmTJdE3+/fv16xZs3Tq1Cldcskl2rp1q2644YbUL4tS7RdJam1t1bvvvquOjo5B20r9c1NbW6vXXntNEydO1GeffaZVq1Zp9uzZOnDggC99E8gQOicUCg147JwbtA5fK/W+euyxx/Tee+/pn//856Btpdo31113nfbt26cvvvhCf/rTn7Rw4UK1t7entpdqv3R1dWnJkiXatm2bLr744iH3K9X+aWhoSP08ZcoUzZo1S9dee61aWlo0c+ZMSbntm0D+Oe7yyy/XRRddNGjU09PTMyiBS925qpVS7qvHH39cb775pt55550BtwEp9b4ZNWqUJkyYoBkzZqi5uVnTpk3T888/X/L9smfPHvX09Gj69OkqKytTWVmZ2tvb9cILL6isrCzVB6XaP982duxYTZkyRQcPHvTlsxPIEBo1apSmT5+utra2Aevb2to0e/Zso1YFU3V1tWKx2IC+6u/vV3t7e9H3lXNOjz32mF5//XX9/e9/V3V19YDtpdw35+OcU19fX8n3yx133KH9+/dr3759qWXGjBl68MEHtW/fPl1zzTUl3T/f1tfXpw8++EDjxo3z57PjqZwhD1pbW93IkSPdyy+/7N5//33X2Njoxo4d6z7++GPrpuVdb2+v27t3r9u7d6+T5NatW+f27t3rDh8+7JxzbvXq1S4SibjXX3/d7d+/3z3wwANu3LhxLplMGrfcX48++qiLRCJu+/bt7ujRo6nlxIkTqX1KtW+WL1/uduzY4To7O917773nnn76aTdixAi3bds251zp9stQvlkd51xp98/Pf/5zt337dnfo0CG3a9cu96Mf/ciVl5enfvfmum8CG0LOObdx40Z39dVXu1GjRrmbb745VXpbat555x0nadCycOFC59zXZZPPPPOMi8ViLhwOu9tuu83t37/fttF5cL4+keReeeWV1D6l2jc/+9nPUt+dK664wt1xxx2pAHKudPtlKN8OoVLun/vuu8+NGzfOjRw50lVWVrr58+e7AwcOpLbnum+4lQMAwEwg/ycEACgNhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzPw/Z1czh0pmPwkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_train[20010])\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "plt.imshow(X_train[0])\n",
    "print(X_train.reshape(-1, 1, 52,52).shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T06:58:23.488967Z",
     "iopub.status.busy": "2025-02-06T06:58:23.488147Z",
     "iopub.status.idle": "2025-02-06T06:58:25.928798Z",
     "shell.execute_reply": "2025-02-06T06:58:25.927452Z",
     "shell.execute_reply.started": "2025-02-06T06:58:23.488927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:22:13.387490Z",
     "iopub.status.busy": "2025-01-24T03:22:13.387026Z",
     "iopub.status.idle": "2025-01-24T03:22:13.396637Z",
     "shell.execute_reply": "2025-01-24T03:22:13.395627Z",
     "shell.execute_reply.started": "2025-01-24T03:22:13.387460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VGGGrayscale(nn.Module):\n",
    "    def __init__(self, num_classes=1000, input_size=(224, 224)):\n",
    "        super(VGGGrayscale, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Additional blocks...\n",
    "        )\n",
    "\n",
    "        # Dynamically calculate flattened size\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.randn(1, 1, *input_size)\n",
    "            flattened_size = self.features(sample_input).view(-1).size(0)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def VGG(num_classes): \n",
    "    model = VGGGrayscale(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:30:16.749580Z",
     "iopub.status.busy": "2025-01-23T06:30:16.749189Z",
     "iopub.status.idle": "2025-01-23T06:30:16.755839Z",
     "shell.execute_reply": "2025-01-23T06:30:16.754487Z",
     "shell.execute_reply.started": "2025-01-23T06:30:16.749546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## mobilenet_v2\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# MobileNetV2 with support for 1-channel input\n",
    "def mobilenet_V2(num_classes):\n",
    "    model = models.mobilenet_v2(pretrained=False)  # Load MobileNetV2 architecture\n",
    "\n",
    "    # Modify the first convolutional layer to accept 1-channel input\n",
    "    model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    # Replace the classifier to match the number of classes in your dataset\n",
    "    model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T06:00:22.480795Z",
     "iopub.status.busy": "2025-01-24T06:00:22.480183Z",
     "iopub.status.idle": "2025-01-24T06:00:22.486770Z",
     "shell.execute_reply": "2025-01-24T06:00:22.485689Z",
     "shell.execute_reply.started": "2025-01-24T06:00:22.480761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MobileNetV3 (Small) with support for 1-channel input\n",
    "def mobilenet_v3_small_single_channel(num_classes):\n",
    "    model = models.mobilenet_v3_small(pretrained=False)  # Load MobileNetV3-Small architecture\n",
    "\n",
    "    # Modify the first convolutional layer to accept 1-channel input\n",
    "    model.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    # Replace the classifier to match the number of classes in your dataset\n",
    "    model.classifier[3] = nn.Linear(model.classifier[0].out_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T08:05:39.204891Z",
     "iopub.status.busy": "2025-01-24T08:05:39.204277Z",
     "iopub.status.idle": "2025-01-24T08:05:39.211944Z",
     "shell.execute_reply": "2025-01-24T08:05:39.210663Z",
     "shell.execute_reply.started": "2025-01-24T08:05:39.204852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MobileNetV3 (Large) with support for 1-channel input\n",
    "def mobilenet_v3_large_single_channel(num_classes):\n",
    "    model = models.mobilenet_v3_large(pretrained=False)  # Load MobileNetV3-Large architecture\n",
    "\n",
    "    # Modify the first convolutional layer to accept 1-channel input\n",
    "    model.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    # Replace the classifier to match the number of classes in your dataset\n",
    "    model.classifier[3] = nn.Linear(model.classifier[0].out_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T01:19:26.424887Z",
     "iopub.status.busy": "2025-01-22T01:19:26.423642Z",
     "iopub.status.idle": "2025-01-22T01:19:26.430750Z",
     "shell.execute_reply": "2025-01-22T01:19:26.429685Z",
     "shell.execute_reply.started": "2025-01-22T01:19:26.424843Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##  mobilenet_ssd\n",
    "import torch\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "\n",
    "# Use pre-built MobileNet-SSD from torchvision\n",
    "\n",
    "def mobilenet_ssd(num_classes):\n",
    "    model = ssd300_vgg16(pretrained=False)  # Load SSD300 with VGG16 backbone\n",
    "    # Replace the head of the model to match the number of classes in your dataset\n",
    "    model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "    # Adjust input channels to 1 (grayscale) instead of 3 (RGB)\n",
    "    model.backbone[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionNext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:30:20.125996Z",
     "iopub.status.busy": "2025-01-23T06:30:20.125606Z",
     "iopub.status.idle": "2025-01-23T06:30:20.142088Z",
     "shell.execute_reply": "2025-01-23T06:30:20.140863Z",
     "shell.execute_reply.started": "2025-01-23T06:30:20.125960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InceptionNextBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a single InceptionNext block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionNextBlock, self).__init__()\n",
    "        mid_channels = out_channels // 4\n",
    "        \n",
    "        # Branch 1: 1x1 Convolution\n",
    "        self.branch1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # Branch 2: 1x1 Convolution followed by 3x3 Convolution\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Branch 3: 1x1 Convolution followed by 5x5 Convolution\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size=5, stride=1, padding=2)\n",
    "        )\n",
    "        \n",
    "        # Branch 4: MaxPooling followed by 1x1 Convolution\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "        # Final pointwise convolution to combine all branches\n",
    "        self.final_conv = nn.Conv2d(mid_channels * 4, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate outputs from all branches\n",
    "        x = torch.cat((self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)), dim=1)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "class InceptionNext(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the InceptionNext architecture for grayscale images.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(InceptionNext, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)  # Grayscale input (1 channel)\n",
    "        self.initial_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # InceptionNext blocks\n",
    "        self.block1 = InceptionNextBlock(64, 128)\n",
    "        self.block2 = InceptionNextBlock(128, 256)\n",
    "        self.block3 = InceptionNextBlock(256, 512)\n",
    "        \n",
    "        # Global Average Pooling and Fully Connected Layer\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.initial_pool(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def inceptionnext(num_classes): \n",
    "    model = InceptionNext(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:30:27.007537Z",
     "iopub.status.busy": "2025-01-23T06:30:27.006300Z",
     "iopub.status.idle": "2025-01-23T06:30:27.015565Z",
     "shell.execute_reply": "2025-01-23T06:30:27.014315Z",
     "shell.execute_reply.started": "2025-01-23T06:30:27.007490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models import convnext_tiny\n",
    "\n",
    "class ConvNeXtGrayscale(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ConvNeXtGrayscale, self).__init__()\n",
    "        # Load pretrained ConvNeXt-Tiny model\n",
    "        self.convnext = convnext_tiny(pretrained=True)\n",
    "        \n",
    "        # Modify the first convolutional layer to accept 1-channel input\n",
    "        self.convnext.features[0][0] = nn.Conv2d(\n",
    "            in_channels=1,  # Grayscale input\n",
    "            out_channels=self.convnext.features[0][0].out_channels,\n",
    "            kernel_size=self.convnext.features[0][0].kernel_size,\n",
    "            stride=self.convnext.features[0][0].stride,\n",
    "            padding=self.convnext.features[0][0].padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Replace the classifier head to match the number of classes\n",
    "        in_features = self.convnext.classifier[2].in_features\n",
    "        self.convnext.classifier[2] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnext(x)\n",
    "\n",
    "def convnext(num_classes): \n",
    "    model = ConvNeXtGrayscale(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:30:29.364725Z",
     "iopub.status.busy": "2025-01-23T06:30:29.363768Z",
     "iopub.status.idle": "2025-01-23T06:30:29.383079Z",
     "shell.execute_reply": "2025-01-23T06:30:29.381802Z",
     "shell.execute_reply.started": "2025-01-23T06:30:29.364672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4  # Output channels will be 4 times the input channels\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet101Modified(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet101Modified, self).__init__()\n",
    "        self.model = models.resnet101(pretrained=True)\n",
    "        \n",
    "        # Modify the first convolution layer to accept 1-channel input\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def resnet101(num_classes): \n",
    "    model = ResNet101Modified(num_classes=num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:19:24.487776Z",
     "iopub.status.busy": "2025-01-23T01:19:24.487028Z",
     "iopub.status.idle": "2025-01-23T01:19:24.499523Z",
     "shell.execute_reply": "2025-01-23T01:19:24.498520Z",
     "shell.execute_reply.started": "2025-01-23T01:19:24.487744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ResNet101(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet101, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Initial Conv and MaxPooling layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(Bottleneck, 64, blocks=3, stride=1)\n",
    "        self.layer2 = self._make_layer(Bottleneck, 128, blocks=4, stride=2)\n",
    "        self.layer3 = self._make_layer(Bottleneck, 256, blocks=23, stride=2)\n",
    "        self.layer4 = self._make_layer(Bottleneck, 512, blocks=3, stride=2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * Bottleneck.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:30:32.417350Z",
     "iopub.status.busy": "2025-01-23T06:30:32.416446Z",
     "iopub.status.idle": "2025-01-23T06:30:36.003443Z",
     "shell.execute_reply": "2025-01-23T06:30:36.002063Z",
     "shell.execute_reply.started": "2025-01-23T06:30:32.417304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BlipProcessor, BlipForImageTextRetrieval\n",
    "from PIL import Image\n",
    "\n",
    "class BLIP(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BLIP, self).__init__()\n",
    "        # Load BLIP pre-trained model\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "        self.blip_model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n",
    "        \n",
    "        # Add a custom classifier head\n",
    "        in_features = self.blip_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x, text=None):\n",
    "        # Preprocess the input image and text\n",
    "        inputs = self.processor(images=x, text=text, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Pass input through BLIP model\n",
    "        outputs = self.blip_model.vision_model(**inputs)\n",
    "        pooled_output = outputs.pooler_output  # Extract pooled output\n",
    "        \n",
    "        # Pass through the custom classifier\n",
    "        output = self.classifier(pooled_output)\n",
    "        return output\n",
    "\n",
    "def blip(num_classes):\n",
    "    \"\"\"\n",
    "    Returns the BLIP model with a classification head.\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes.\n",
    "    \"\"\"\n",
    "    model = BLIP(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:32:18.206488Z",
     "iopub.status.busy": "2025-01-24T03:32:18.205611Z",
     "iopub.status.idle": "2025-01-24T03:32:28.402527Z",
     "shell.execute_reply": "2025-01-24T03:32:28.401479Z",
     "shell.execute_reply.started": "2025-01-24T03:32:18.206449Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-3yeb9dz7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-3yeb9dz7\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (23.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (1.13.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (0.14.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->clip==1.0) (0.2.6)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->clip==1.0) (4.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (1.21.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (9.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:32:41.246096Z",
     "iopub.status.busy": "2025-01-24T03:32:41.245003Z",
     "iopub.status.idle": "2025-01-24T03:32:41.254998Z",
     "shell.execute_reply": "2025-01-24T03:32:41.253883Z",
     "shell.execute_reply.started": "2025-01-24T03:32:41.246053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Clip\n",
    "import clip\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        # Load the CLIP model\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Modify the classifier head\n",
    "        self.classifier = nn.Linear(self.clip_model.visual.output_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "    # Ensure input is not None and has valid dimensions\n",
    "        if x is None:\n",
    "            raise ValueError(\"Input tensor is None.\")\n",
    "        if len(x.size()) != 4:\n",
    "            raise ValueError(f\"Expected input shape [batch_size, channels, height, width], but got {x.size()}.\")\n",
    "    \n",
    "    # Convert 1-channel input to 3 channels if necessary\n",
    "        if x.shape[1] == 1:  # Single channel (grayscale)\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "    \n",
    "        print(f\"Input shape to encode_image: {x.size()}\")  # Debug\n",
    "    # Pass the image through CLIP's encode_image\n",
    "        visual_features = self.clip_model.encode_image(x)\n",
    "        print(f\"Encoded features shape: {visual_features.size()}\")  # Debug\n",
    "\n",
    "    # Classify the features\n",
    "        output = self.classifier(visual_features)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "def clip_model(num_classes):\n",
    "    model = CustomCLIPModel(num_classes=num_classes)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:30:49.175281Z",
     "iopub.status.busy": "2025-01-23T06:30:49.174933Z",
     "iopub.status.idle": "2025-01-23T06:30:49.186886Z",
     "shell.execute_reply": "2025-01-23T06:30:49.185509Z",
     "shell.execute_reply.started": "2025-01-23T06:30:49.175248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(EfficientNetGrayscale, self).__init__()\n",
    "        # Load pretrained EfficientNet-B0 model\n",
    "        self.efficientnet = efficientnet_b7(pretrained=True)\n",
    "        \n",
    "        # Modify the first convolutional layer to accept 1-channel input\n",
    "        self.efficientnet.features[0][0] = nn.Conv2d(\n",
    "            in_channels=1,  # Grayscale input\n",
    "            out_channels=self.efficientnet.features[0][0].out_channels,\n",
    "            kernel_size=self.efficientnet.features[0][0].kernel_size,\n",
    "            stride=self.efficientnet.features[0][0].stride,\n",
    "            padding=self.efficientnet.features[0][0].padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Replace the classifier head to match the number of classes\n",
    "        in_features = self.efficientnet.classifier[1].in_features\n",
    "        self.efficientnet.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n",
    "\n",
    "\n",
    "def efficientnet(num_classes):\n",
    "    model = EfficientNet(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:19:52.329177Z",
     "iopub.status.busy": "2025-01-23T01:19:52.328555Z",
     "iopub.status.idle": "2025-01-23T01:19:52.340154Z",
     "shell.execute_reply": "2025-01-23T01:19:52.339135Z",
     "shell.execute_reply.started": "2025-01-23T01:19:52.329140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# EfficientNet with support for 1-channel input\n",
    "def efficientnet_b0(num_classes):\n",
    "    model = models.efficientnet_b0(pretrained=False)  # Load EfficientNet-B0 architecture\n",
    "\n",
    "    # Modify the first convolutional layer to accept 1-channel input\n",
    "    model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    # Replace the classifier to match the number of classes in your dataset\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:30:49.189274Z",
     "iopub.status.busy": "2025-01-23T06:30:49.188907Z",
     "iopub.status.idle": "2025-01-23T06:30:49.207687Z",
     "shell.execute_reply": "2025-01-23T06:30:49.206603Z",
     "shell.execute_reply.started": "2025-01-23T06:30:49.189236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# EfficientNet-B7 with support for 1-channel input\n",
    "def efficientnet_b7(num_classes):\n",
    "    model = models.efficientnet_b7(pretrained=False)  # Load EfficientNet-B7 architecture\n",
    "\n",
    "    # Modify the first convolutional layer to accept 1-channel input\n",
    "    model.features[0][0] = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    # Replace the classifier to match the number of classes in your dataset\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T06:31:07.423265Z",
     "iopub.status.busy": "2025-01-23T06:31:07.422269Z",
     "iopub.status.idle": "2025-01-23T06:31:07.436155Z",
     "shell.execute_reply": "2025-01-23T06:31:07.434960Z",
     "shell.execute_reply.started": "2025-01-23T06:31:07.423214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# from torchsummary import summary\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, init_weights=False):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=3, stride=1, padding=1),  # input[1, 52, 52]  output[128, 52, 52]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),                  # output[128, 25, 25]\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=5, padding=10),           # output[256, 41, 41]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),                  # output[256, 20, 20]\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),          # output[384, 20, 20]\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),          # output[384, 20, 20]\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),          # output[256, 20, 20]\n",
    "            \n",
    "            nn.ReLU(inplace=True),\n",
    "             nn.MaxPool2d(kernel_size=3, stride=2),                  # output[256, 9, 9]\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256 * 9 * 9, 4096), # output[65536, 4096]\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "      #  print(x.shape)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def alexnet(num_classes): \n",
    "    model = AlexNet(num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# net = AlexNet(num_classes=5)\n",
    "# summary(net.to('cuda'), (3,224,224))\n",
    "#########################################################################################################################################\n",
    "# Total params: 62,378,344\n",
    "# Trainable params: 62,378,344\n",
    "# Non-trainable params: 0\n",
    "# ----------------------------------------------------------------\n",
    "# Input size (MB): 0.57\n",
    "# Forward/backward pass size (MB): 11.09\n",
    "# Params size (MB): 237.95\n",
    "# Estimated Total Size (MB): 249.62\n",
    "# ----------------------------------------------------------------\n",
    "# conv_parameters:  3,747,200\n",
    "# fnn_parameters:  58,631,144   93% 的参数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T14:39:06.719898Z",
     "iopub.status.busy": "2025-01-22T14:39:06.719015Z",
     "iopub.status.idle": "2025-01-22T14:39:06.730816Z",
     "shell.execute_reply": "2025-01-22T14:39:06.729789Z",
     "shell.execute_reply.started": "2025-01-22T14:39:06.719862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, img_size=52, patch_size=4, num_channels=1, d_model=256, nhead=8, num_encoder_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        \n",
    "        # Patch embedding layer (similar to Conv2d)\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.patch_embed = nn.Conv2d(num_channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, self.num_patches, d_model))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Dropout and Regularization to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches and flatten\n",
    "        batch_size = x.size(0)\n",
    "        x = self.patch_embed(x)  # Shape: [batch_size, d_model, H/p, W/p]\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, d_model]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        # Pass through Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # Shape: [batch_size, num_patches, d_model]\n",
    "\n",
    "        # Apply BatchNorm and Dropout\n",
    "        x = x.mean(dim=1)  # Shape: [batch_size, d_model]\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Classification\n",
    "        x = self.classifier(x)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        return x\n",
    "\n",
    "def transformer_net(num_classes):\n",
    "    model = TransformerNet(num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# Example instantiation\n",
    "# net = TransformerNet(num_classes=5, img_size=52, patch_size=4)\n",
    "# summary(net.to('cuda'), (1, 52, 52))\n",
    "#########################################################################################################################################\n",
    "# Total params: (Dependent on d_model and Transformer configurations)\n",
    "# Trainable params: (Dependent on d_model and Transformer configurations)\n",
    "# Non-trainable params: 0\n",
    "# ----------------------------------------------------------------\n",
    "# Input size (MB): Calculated dynamically\n",
    "# Forward/backward pass size (MB): Calculated dynamically\n",
    "# Params size (MB): Calculated dynamically\n",
    "# Estimated Total Size (MB): Calculated dynamically\n",
    "# ----------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:53:58.767978Z",
     "iopub.status.busy": "2025-01-24T01:53:58.767353Z",
     "iopub.status.idle": "2025-01-24T01:53:59.813148Z",
     "shell.execute_reply": "2025-01-24T01:53:59.811829Z",
     "shell.execute_reply.started": "2025-01-24T01:53:58.767938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models import create_model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, num_classes=1000, img_size=224):\n",
    "        super(ViTModel, self).__init__()\n",
    "        # Convert grayscale to 3-channel\n",
    "        self.input_adapter = nn.Conv2d(1, 3, kernel_size=1)  # Converts [1, H, W] to [3, H, W]\n",
    "        \n",
    "        # Vision Transformer backbone\n",
    "        self.backbone = create_model(\n",
    "            'vit_small_patch16_224',  # Pre-trained ViT model from timm\n",
    "            pretrained=True,\n",
    "            num_classes=num_classes,\n",
    "            img_size=img_size,\n",
    "            in_chans=3,  # Input channels (after adapter)\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Resize input to match model requirements\n",
    "        x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)\n",
    "        x = self.input_adapter(x)  # Convert grayscale to RGB\n",
    "        x = self.backbone(x)       # Pass through the ViT model\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "def vit_model(num_classes):\n",
    "    model = ViTModel(num_classes=num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T07:01:16.535368Z",
     "iopub.status.busy": "2025-02-06T07:01:16.534958Z",
     "iopub.status.idle": "2025-02-06T07:01:39.066857Z",
     "shell.execute_reply": "2025-02-06T07:01:39.065917Z",
     "shell.execute_reply.started": "2025-02-06T07:01:16.535327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models import create_model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, model_name='vit_base_patch16_224', num_classes=1000, img_size=224):\n",
    "        super(ViTModel, self).__init__()\n",
    "        \n",
    "        # Convert grayscale to 3-channel\n",
    "        self.input_adapter = nn.Conv2d(1, 3, kernel_size=1)  # Converts [1, H, W] to [3, H, W]\n",
    "        \n",
    "        # Vision Transformer backbone\n",
    "        self.backbone = create_model(\n",
    "            model_name,  # Choose ViT variant\n",
    "            pretrained=True,\n",
    "            num_classes=num_classes,\n",
    "            img_size=img_size,\n",
    "            in_chans=3,  # Input channels (after adapter)\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Resize input to match model requirements\n",
    "        x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)\n",
    "        x = self.input_adapter(x)  # Convert grayscale to RGB\n",
    "        x = self.backbone(x)       # Pass through the ViT model\n",
    "        return x\n",
    "\n",
    "# Function to instantiate different ViT models\n",
    "def vit_model(variant='base', num_classes=38):\n",
    "    model_dict = {\n",
    "        'base': 'vit_base_patch16_224',\n",
    "        'large': 'vit_large_patch16_224',\n",
    "        'huge': 'vit_huge_patch14_224'\n",
    "    }\n",
    "    model_name = model_dict.get(variant, 'vit_base_patch16_224')\n",
    "    return ViTModel(model_name=model_name, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:28:07.896663Z",
     "iopub.status.busy": "2025-01-24T02:28:07.896209Z",
     "iopub.status.idle": "2025-01-24T02:28:08.968508Z",
     "shell.execute_reply": "2025-01-24T02:28:08.967566Z",
     "shell.execute_reply.started": "2025-01-24T02:28:07.896607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#To adapt the Vision Transformer (ViT) model for few-shot learning, \n",
    "#you can integrate a prototype-based approach. \n",
    "#Few-shot learning typically involves learning representations \n",
    "#from a small number of labeled examples and using them to classify unseen samples.\n",
    "#Below is an example implementation of a prototype-based few-shot \n",
    "#learning model using the ViT backbone:\n",
    "#Few-Shot Learning Model Code\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models import create_model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FewShotViTModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, img_size=224):\n",
    "        super(FewShotViTModel, self).__init__()\n",
    "        # Convert grayscale to 3-channel\n",
    "        self.input_adapter = nn.Conv2d(1, 3, kernel_size=1)  # Converts [1, H, W] to [3, H, W]\n",
    "        \n",
    "        # Vision Transformer backbone\n",
    "        self.backbone = create_model(\n",
    "            'vit_base_patch16_224',  # Pre-trained ViT model from timm\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # Exclude the classifier layer\n",
    "            img_size=img_size,\n",
    "            in_chans=3,  # Input channels (after adapter)\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.feature_dim = self.backbone.embed_dim  # Output dimension of the ViT feature extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Resize input to match model requirements\n",
    "        x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)\n",
    "        x = self.input_adapter(x)  # Convert grayscale to RGB\n",
    "        features = self.backbone(x)  # Extract features using ViT\n",
    "        return features\n",
    "\n",
    "class FewShotLearner:\n",
    "    def __init__(self, model, n_support, n_query):\n",
    "        \"\"\"\n",
    "        Initialize the few-shot learner.\n",
    "\n",
    "        Args:\n",
    "        - model: Feature extractor (e.g., ViTModel).\n",
    "        - n_support: Number of support samples per class.\n",
    "        - n_query: Number of query samples per class.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "\n",
    "    def compute_prototypes(self, support_features, support_labels):\n",
    "        \"\"\"\n",
    "        Compute class prototypes from support set.\n",
    "\n",
    "        Args:\n",
    "        - support_features: Tensor of shape [n_classes * n_support, feature_dim].\n",
    "        - support_labels: Tensor of shape [n_classes * n_support].\n",
    "\n",
    "        Returns:\n",
    "        - prototypes: Tensor of shape [n_classes, feature_dim].\n",
    "        \"\"\"\n",
    "        prototypes = []\n",
    "        n_classes = support_labels.unique().size(0)\n",
    "        for cls in range(n_classes):\n",
    "            cls_features = support_features[support_labels == cls]\n",
    "            prototype = cls_features.mean(dim=0)  # Compute mean feature for each class\n",
    "            prototypes.append(prototype)\n",
    "        return torch.stack(prototypes, dim=0)\n",
    "\n",
    "    def classify(self, query_features, prototypes):\n",
    "        \"\"\"\n",
    "        Classify query samples based on prototypes.\n",
    "\n",
    "        Args:\n",
    "        - query_features: Tensor of shape [n_classes * n_query, feature_dim].\n",
    "        - prototypes: Tensor of shape [n_classes, feature_dim].\n",
    "\n",
    "        Returns:\n",
    "        - logits: Tensor of shape [n_classes * n_query, n_classes].\n",
    "        \"\"\"\n",
    "        # Compute squared Euclidean distance between query features and prototypes\n",
    "        distances = torch.cdist(query_features, prototypes, p=2) ** 2\n",
    "        logits = -distances  # Convert distances to logits (negative for classification)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, support_set, support_labels, query_set, query_labels):\n",
    "        \"\"\"\n",
    "        Perform few-shot learning.\n",
    "\n",
    "        Args:\n",
    "        - support_set: Tensor of shape [n_classes * n_support, 1, H, W].\n",
    "        - support_labels: Tensor of shape [n_classes * n_support].\n",
    "        - query_set: Tensor of shape [n_classes * n_query, 1, H, W].\n",
    "        - query_labels: Tensor of shape [n_classes * n_query].\n",
    "\n",
    "        Returns:\n",
    "        - loss: Few-shot classification loss.\n",
    "        - accuracy: Few-shot classification accuracy.\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        support_features = self.model(support_set)\n",
    "        query_features = self.model(query_set)\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = self.compute_prototypes(support_features, support_labels)\n",
    "\n",
    "        # Classify query set\n",
    "        logits = self.classify(query_features, prototypes)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, query_labels)\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predictions = logits.max(dim=1)\n",
    "        accuracy = (predictions == query_labels).float().mean()\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "# Instantiate the model and few-shot learner\n",
    "def few_shot_vit_model(n_support, n_query, num_classes):\n",
    "    model = FewShotViTModel(num_classes=num_classes)\n",
    "    learner = FewShotLearner(model, n_support=n_support, n_query=n_query)\n",
    "    return learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue working    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T07:01:39.068380Z",
     "iopub.status.busy": "2025-02-06T07:01:39.068101Z",
     "iopub.status.idle": "2025-02-06T07:01:39.083604Z",
     "shell.execute_reply": "2025-02-06T07:01:39.082383Z",
     "shell.execute_reply.started": "2025-02-06T07:01:39.068353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class My_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, transform = None):\n",
    "#         self.data_path = data_path\n",
    "#         self.data = np.load(data_path)\n",
    "        self.images = X.reshape(-1,1, 52, 52)\n",
    "        self.one_hot_labels = y\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.classes_dict = {\n",
    "            \"00000000\":0,\n",
    "            \"10000000\":1,\n",
    "            \"01000000\":2,\n",
    "            \"00100000\":3,\n",
    "            \"00010000\":4,\n",
    "            \"00001000\":5,\n",
    "            \"00000100\":6,\n",
    "            \"00000010\":7,\n",
    "            \"00000001\":8,\n",
    "            \"10100000\":9,\n",
    "            \"10010000\":10,\n",
    "            \"10001000\":11,\n",
    "            \"10000010\":12,\n",
    "            \"01100000\":13,\n",
    "            \"01010000\":14,\n",
    "            \"01001000\":15,\n",
    "            \"01000010\":16,\n",
    "            \"00101000\":17,\n",
    "            \"00100010\":18,\n",
    "            \"00011000\":19,\n",
    "            \"00010010\":20,\n",
    "            \"00001010\":21,\n",
    "            \"10101000\":22,\n",
    "            \"10100010\":23,\n",
    "            \"10011000\":24,\n",
    "            \"10010010\":25,\n",
    "            \"10001010\":26,\n",
    "            \"01101000\":27,\n",
    "            \"01100010\":28,\n",
    "            \"01011000\":29,\n",
    "            \"01010010\":30,\n",
    "            \"01001010\":31,\n",
    "            \"00101010\":32,\n",
    "            \"00011010\":33,\n",
    "            \"10101010\":34,\n",
    "            \"10011010\":35,\n",
    "            \"01101010\":36,\n",
    "            \"01011010\":37,\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.classes_num_dict = {}\n",
    "        \n",
    "        json_str = json.dumps(self.classes_dict, indent = 4)\n",
    "        with open( \"classes_dict.json\" , 'w') as json_file:\n",
    "            json_file.write(json_str)\n",
    "        \n",
    "        \n",
    "        self.image_labels = []\n",
    "        self.images_num = []\n",
    "        \n",
    "        cnt = 0\n",
    "        for one_hot_label in self.one_hot_labels:\n",
    "            cnt += 1\n",
    "            t = \"\"\n",
    "            for ch in one_hot_label:\n",
    "                if ch == None:\n",
    "                    t += str(0)\n",
    "                else:\n",
    "                    t += str(ch)\n",
    "            label = self.classes_dict[t]\n",
    "            if label  not in self.classes_num_dict:\n",
    "                self.classes_num_dict[label] = 1\n",
    "            else:\n",
    "                self.classes_num_dict[label] += 1\n",
    "            self.image_labels.append(label)\n",
    "            \n",
    "        for i in sorted(list(self.classes_num_dict.keys())):\n",
    "            self.images_num.append(self.classes_num_dict[i])\n",
    "        \n",
    "        self.num_classes = len(self.classes_num_dict)\n",
    "        \n",
    "        print(\"num_class：\", self.num_classes)\n",
    "        print(\"{}/{} images were  found in the dataset.\".format(cnt, sum(self.images_num)))\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return sum(self.images_num)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.image_labels[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T07:01:39.086106Z",
     "iopub.status.busy": "2025-02-06T07:01:39.085799Z",
     "iopub.status.idle": "2025-02-06T07:01:39.377997Z",
     "shell.execute_reply": "2025-02-06T07:01:39.376778Z",
     "shell.execute_reply.started": "2025-02-06T07:01:39.086077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class： 38\n",
      "30412/30412 images were  found in the dataset.\n",
      "num_class： 38\n",
      "7603/7603 images were  found in the dataset.\n",
      "<class 'numpy.ndarray'>\n",
      "int32\n",
      "(30412, 52, 52)\n",
      "(7603, 52, 52)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "# data_transform = {\n",
    "#     \"train\": transforms.Compose([transforms.ToTensor(),\n",
    "#                                  transforms.Resize((224,224)),\n",
    "#                                  transforms.CenterCrop(224),\n",
    "#                                  transforms.Normalize((0.5,), (0.5,))]),\n",
    "#     \"val\": transforms.Compose([transforms.ToTensor(),\n",
    "#                                transforms.Resize((224,224)),\n",
    "#                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# }\n",
    "train_dataset = My_Dataset(X_train, y_train)\n",
    "test_dataset = My_Dataset(X_test, y_test)\n",
    "val_num = len(test_dataset)\n",
    "\n",
    "print(type(train_dataset[0][0]))\n",
    "print(train_dataset[0][0].dtype)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# print(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T07:01:39.379792Z",
     "iopub.status.busy": "2025-02-06T07:01:39.379319Z",
     "iopub.status.idle": "2025-02-06T07:01:39.537919Z",
     "shell.execute_reply": "2025-02-06T07:01:39.536277Z",
     "shell.execute_reply.started": "2025-02-06T07:01:39.379744Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476\n",
      "1901\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "dataloader自动转化为torch.int32类型数据，但训练时需要的是float类型计算\n",
    "\"\"\"\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=4, shuffle=False, num_workers=2, drop_last=False)\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "\n",
    "#iters = iter(train_loader)\n",
    "#a, b = next(iters)\n",
    "#print(type(a))\n",
    "#print(a.dtype) \n",
    "#print(b.shape)\n",
    "\n",
    "# for data in train_loader:\n",
    "#     imgs, lab = data\n",
    "#     print(imgs.dtype)\n",
    "#     print(lab.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-10T08:30:01.702085Z",
     "iopub.status.idle": "2023-04-10T08:30:01.702681Z",
     "shell.execute_reply": "2023-04-10T08:30:01.702446Z",
     "shell.execute_reply.started": "2023-04-10T08:30:01.702405Z"
    }
   },
   "source": [
    "\n",
    "1、实例化模型   \n",
    "2、定义损失函数，优化器   \n",
    "3、定义迭代次数   \n",
    "4、查询是否保存模型参数   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T07:01:39.541811Z",
     "iopub.status.busy": "2025-02-06T07:01:39.540672Z",
     "iopub.status.idle": "2025-02-06T07:01:51.954278Z",
     "shell.execute_reply": "2025-02-06T07:01:51.951613Z",
     "shell.execute_reply.started": "2025-02-06T07:01:39.541743Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\kim\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yarrow (c:\\Users\\kim\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T07:01:51.958380Z",
     "iopub.status.busy": "2025-02-06T07:01:51.957821Z",
     "iopub.status.idle": "2025-02-06T07:01:51.968228Z",
     "shell.execute_reply": "2025-02-06T07:01:51.966928Z",
     "shell.execute_reply.started": "2025-02-06T07:01:51.958327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T07:02:18.964191Z",
     "iopub.status.busy": "2025-02-06T07:02:18.962939Z",
     "iopub.status.idle": "2025-02-06T07:02:21.213134Z",
     "shell.execute_reply": "2025-02-06T07:02:21.211448Z",
     "shell.execute_reply.started": "2025-02-06T07:02:18.964147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 224, 224]               6\n",
      "            Conv2d-2          [-1, 768, 14, 14]         590,592\n",
      "          Identity-3             [-1, 196, 768]               0\n",
      "        PatchEmbed-4             [-1, 196, 768]               0\n",
      "           Dropout-5             [-1, 197, 768]               0\n",
      "          Identity-6             [-1, 197, 768]               0\n",
      "          Identity-7             [-1, 197, 768]               0\n",
      "         LayerNorm-8             [-1, 197, 768]           1,536\n",
      "            Linear-9            [-1, 197, 2304]       1,771,776\n",
      "         Identity-10          [-1, 12, 197, 64]               0\n",
      "         Identity-11          [-1, 12, 197, 64]               0\n",
      "           Linear-12             [-1, 197, 768]         590,592\n",
      "          Dropout-13             [-1, 197, 768]               0\n",
      "        Attention-14             [-1, 197, 768]               0\n",
      "         Identity-15             [-1, 197, 768]               0\n",
      "         Identity-16             [-1, 197, 768]               0\n",
      "        LayerNorm-17             [-1, 197, 768]           1,536\n",
      "           Linear-18            [-1, 197, 3072]       2,362,368\n",
      "             GELU-19            [-1, 197, 3072]               0\n",
      "          Dropout-20            [-1, 197, 3072]               0\n",
      "         Identity-21            [-1, 197, 3072]               0\n",
      "           Linear-22             [-1, 197, 768]       2,360,064\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "              Mlp-24             [-1, 197, 768]               0\n",
      "         Identity-25             [-1, 197, 768]               0\n",
      "         Identity-26             [-1, 197, 768]               0\n",
      "            Block-27             [-1, 197, 768]               0\n",
      "        LayerNorm-28             [-1, 197, 768]           1,536\n",
      "           Linear-29            [-1, 197, 2304]       1,771,776\n",
      "         Identity-30          [-1, 12, 197, 64]               0\n",
      "         Identity-31          [-1, 12, 197, 64]               0\n",
      "           Linear-32             [-1, 197, 768]         590,592\n",
      "          Dropout-33             [-1, 197, 768]               0\n",
      "        Attention-34             [-1, 197, 768]               0\n",
      "         Identity-35             [-1, 197, 768]               0\n",
      "         Identity-36             [-1, 197, 768]               0\n",
      "        LayerNorm-37             [-1, 197, 768]           1,536\n",
      "           Linear-38            [-1, 197, 3072]       2,362,368\n",
      "             GELU-39            [-1, 197, 3072]               0\n",
      "          Dropout-40            [-1, 197, 3072]               0\n",
      "         Identity-41            [-1, 197, 3072]               0\n",
      "           Linear-42             [-1, 197, 768]       2,360,064\n",
      "          Dropout-43             [-1, 197, 768]               0\n",
      "              Mlp-44             [-1, 197, 768]               0\n",
      "         Identity-45             [-1, 197, 768]               0\n",
      "         Identity-46             [-1, 197, 768]               0\n",
      "            Block-47             [-1, 197, 768]               0\n",
      "        LayerNorm-48             [-1, 197, 768]           1,536\n",
      "           Linear-49            [-1, 197, 2304]       1,771,776\n",
      "         Identity-50          [-1, 12, 197, 64]               0\n",
      "         Identity-51          [-1, 12, 197, 64]               0\n",
      "           Linear-52             [-1, 197, 768]         590,592\n",
      "          Dropout-53             [-1, 197, 768]               0\n",
      "        Attention-54             [-1, 197, 768]               0\n",
      "         Identity-55             [-1, 197, 768]               0\n",
      "         Identity-56             [-1, 197, 768]               0\n",
      "        LayerNorm-57             [-1, 197, 768]           1,536\n",
      "           Linear-58            [-1, 197, 3072]       2,362,368\n",
      "             GELU-59            [-1, 197, 3072]               0\n",
      "          Dropout-60            [-1, 197, 3072]               0\n",
      "         Identity-61            [-1, 197, 3072]               0\n",
      "           Linear-62             [-1, 197, 768]       2,360,064\n",
      "          Dropout-63             [-1, 197, 768]               0\n",
      "              Mlp-64             [-1, 197, 768]               0\n",
      "         Identity-65             [-1, 197, 768]               0\n",
      "         Identity-66             [-1, 197, 768]               0\n",
      "            Block-67             [-1, 197, 768]               0\n",
      "        LayerNorm-68             [-1, 197, 768]           1,536\n",
      "           Linear-69            [-1, 197, 2304]       1,771,776\n",
      "         Identity-70          [-1, 12, 197, 64]               0\n",
      "         Identity-71          [-1, 12, 197, 64]               0\n",
      "           Linear-72             [-1, 197, 768]         590,592\n",
      "          Dropout-73             [-1, 197, 768]               0\n",
      "        Attention-74             [-1, 197, 768]               0\n",
      "         Identity-75             [-1, 197, 768]               0\n",
      "         Identity-76             [-1, 197, 768]               0\n",
      "        LayerNorm-77             [-1, 197, 768]           1,536\n",
      "           Linear-78            [-1, 197, 3072]       2,362,368\n",
      "             GELU-79            [-1, 197, 3072]               0\n",
      "          Dropout-80            [-1, 197, 3072]               0\n",
      "         Identity-81            [-1, 197, 3072]               0\n",
      "           Linear-82             [-1, 197, 768]       2,360,064\n",
      "          Dropout-83             [-1, 197, 768]               0\n",
      "              Mlp-84             [-1, 197, 768]               0\n",
      "         Identity-85             [-1, 197, 768]               0\n",
      "         Identity-86             [-1, 197, 768]               0\n",
      "            Block-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "         Identity-90          [-1, 12, 197, 64]               0\n",
      "         Identity-91          [-1, 12, 197, 64]               0\n",
      "           Linear-92             [-1, 197, 768]         590,592\n",
      "          Dropout-93             [-1, 197, 768]               0\n",
      "        Attention-94             [-1, 197, 768]               0\n",
      "         Identity-95             [-1, 197, 768]               0\n",
      "         Identity-96             [-1, 197, 768]               0\n",
      "        LayerNorm-97             [-1, 197, 768]           1,536\n",
      "           Linear-98            [-1, 197, 3072]       2,362,368\n",
      "             GELU-99            [-1, 197, 3072]               0\n",
      "         Dropout-100            [-1, 197, 3072]               0\n",
      "        Identity-101            [-1, 197, 3072]               0\n",
      "          Linear-102             [-1, 197, 768]       2,360,064\n",
      "         Dropout-103             [-1, 197, 768]               0\n",
      "             Mlp-104             [-1, 197, 768]               0\n",
      "        Identity-105             [-1, 197, 768]               0\n",
      "        Identity-106             [-1, 197, 768]               0\n",
      "           Block-107             [-1, 197, 768]               0\n",
      "       LayerNorm-108             [-1, 197, 768]           1,536\n",
      "          Linear-109            [-1, 197, 2304]       1,771,776\n",
      "        Identity-110          [-1, 12, 197, 64]               0\n",
      "        Identity-111          [-1, 12, 197, 64]               0\n",
      "          Linear-112             [-1, 197, 768]         590,592\n",
      "         Dropout-113             [-1, 197, 768]               0\n",
      "       Attention-114             [-1, 197, 768]               0\n",
      "        Identity-115             [-1, 197, 768]               0\n",
      "        Identity-116             [-1, 197, 768]               0\n",
      "       LayerNorm-117             [-1, 197, 768]           1,536\n",
      "          Linear-118            [-1, 197, 3072]       2,362,368\n",
      "            GELU-119            [-1, 197, 3072]               0\n",
      "         Dropout-120            [-1, 197, 3072]               0\n",
      "        Identity-121            [-1, 197, 3072]               0\n",
      "          Linear-122             [-1, 197, 768]       2,360,064\n",
      "         Dropout-123             [-1, 197, 768]               0\n",
      "             Mlp-124             [-1, 197, 768]               0\n",
      "        Identity-125             [-1, 197, 768]               0\n",
      "        Identity-126             [-1, 197, 768]               0\n",
      "           Block-127             [-1, 197, 768]               0\n",
      "       LayerNorm-128             [-1, 197, 768]           1,536\n",
      "          Linear-129            [-1, 197, 2304]       1,771,776\n",
      "        Identity-130          [-1, 12, 197, 64]               0\n",
      "        Identity-131          [-1, 12, 197, 64]               0\n",
      "          Linear-132             [-1, 197, 768]         590,592\n",
      "         Dropout-133             [-1, 197, 768]               0\n",
      "       Attention-134             [-1, 197, 768]               0\n",
      "        Identity-135             [-1, 197, 768]               0\n",
      "        Identity-136             [-1, 197, 768]               0\n",
      "       LayerNorm-137             [-1, 197, 768]           1,536\n",
      "          Linear-138            [-1, 197, 3072]       2,362,368\n",
      "            GELU-139            [-1, 197, 3072]               0\n",
      "         Dropout-140            [-1, 197, 3072]               0\n",
      "        Identity-141            [-1, 197, 3072]               0\n",
      "          Linear-142             [-1, 197, 768]       2,360,064\n",
      "         Dropout-143             [-1, 197, 768]               0\n",
      "             Mlp-144             [-1, 197, 768]               0\n",
      "        Identity-145             [-1, 197, 768]               0\n",
      "        Identity-146             [-1, 197, 768]               0\n",
      "           Block-147             [-1, 197, 768]               0\n",
      "       LayerNorm-148             [-1, 197, 768]           1,536\n",
      "          Linear-149            [-1, 197, 2304]       1,771,776\n",
      "        Identity-150          [-1, 12, 197, 64]               0\n",
      "        Identity-151          [-1, 12, 197, 64]               0\n",
      "          Linear-152             [-1, 197, 768]         590,592\n",
      "         Dropout-153             [-1, 197, 768]               0\n",
      "       Attention-154             [-1, 197, 768]               0\n",
      "        Identity-155             [-1, 197, 768]               0\n",
      "        Identity-156             [-1, 197, 768]               0\n",
      "       LayerNorm-157             [-1, 197, 768]           1,536\n",
      "          Linear-158            [-1, 197, 3072]       2,362,368\n",
      "            GELU-159            [-1, 197, 3072]               0\n",
      "         Dropout-160            [-1, 197, 3072]               0\n",
      "        Identity-161            [-1, 197, 3072]               0\n",
      "          Linear-162             [-1, 197, 768]       2,360,064\n",
      "         Dropout-163             [-1, 197, 768]               0\n",
      "             Mlp-164             [-1, 197, 768]               0\n",
      "        Identity-165             [-1, 197, 768]               0\n",
      "        Identity-166             [-1, 197, 768]               0\n",
      "           Block-167             [-1, 197, 768]               0\n",
      "       LayerNorm-168             [-1, 197, 768]           1,536\n",
      "          Linear-169            [-1, 197, 2304]       1,771,776\n",
      "        Identity-170          [-1, 12, 197, 64]               0\n",
      "        Identity-171          [-1, 12, 197, 64]               0\n",
      "          Linear-172             [-1, 197, 768]         590,592\n",
      "         Dropout-173             [-1, 197, 768]               0\n",
      "       Attention-174             [-1, 197, 768]               0\n",
      "        Identity-175             [-1, 197, 768]               0\n",
      "        Identity-176             [-1, 197, 768]               0\n",
      "       LayerNorm-177             [-1, 197, 768]           1,536\n",
      "          Linear-178            [-1, 197, 3072]       2,362,368\n",
      "            GELU-179            [-1, 197, 3072]               0\n",
      "         Dropout-180            [-1, 197, 3072]               0\n",
      "        Identity-181            [-1, 197, 3072]               0\n",
      "          Linear-182             [-1, 197, 768]       2,360,064\n",
      "         Dropout-183             [-1, 197, 768]               0\n",
      "             Mlp-184             [-1, 197, 768]               0\n",
      "        Identity-185             [-1, 197, 768]               0\n",
      "        Identity-186             [-1, 197, 768]               0\n",
      "           Block-187             [-1, 197, 768]               0\n",
      "       LayerNorm-188             [-1, 197, 768]           1,536\n",
      "          Linear-189            [-1, 197, 2304]       1,771,776\n",
      "        Identity-190          [-1, 12, 197, 64]               0\n",
      "        Identity-191          [-1, 12, 197, 64]               0\n",
      "          Linear-192             [-1, 197, 768]         590,592\n",
      "         Dropout-193             [-1, 197, 768]               0\n",
      "       Attention-194             [-1, 197, 768]               0\n",
      "        Identity-195             [-1, 197, 768]               0\n",
      "        Identity-196             [-1, 197, 768]               0\n",
      "       LayerNorm-197             [-1, 197, 768]           1,536\n",
      "          Linear-198            [-1, 197, 3072]       2,362,368\n",
      "            GELU-199            [-1, 197, 3072]               0\n",
      "         Dropout-200            [-1, 197, 3072]               0\n",
      "        Identity-201            [-1, 197, 3072]               0\n",
      "          Linear-202             [-1, 197, 768]       2,360,064\n",
      "         Dropout-203             [-1, 197, 768]               0\n",
      "             Mlp-204             [-1, 197, 768]               0\n",
      "        Identity-205             [-1, 197, 768]               0\n",
      "        Identity-206             [-1, 197, 768]               0\n",
      "           Block-207             [-1, 197, 768]               0\n",
      "       LayerNorm-208             [-1, 197, 768]           1,536\n",
      "          Linear-209            [-1, 197, 2304]       1,771,776\n",
      "        Identity-210          [-1, 12, 197, 64]               0\n",
      "        Identity-211          [-1, 12, 197, 64]               0\n",
      "          Linear-212             [-1, 197, 768]         590,592\n",
      "         Dropout-213             [-1, 197, 768]               0\n",
      "       Attention-214             [-1, 197, 768]               0\n",
      "        Identity-215             [-1, 197, 768]               0\n",
      "        Identity-216             [-1, 197, 768]               0\n",
      "       LayerNorm-217             [-1, 197, 768]           1,536\n",
      "          Linear-218            [-1, 197, 3072]       2,362,368\n",
      "            GELU-219            [-1, 197, 3072]               0\n",
      "         Dropout-220            [-1, 197, 3072]               0\n",
      "        Identity-221            [-1, 197, 3072]               0\n",
      "          Linear-222             [-1, 197, 768]       2,360,064\n",
      "         Dropout-223             [-1, 197, 768]               0\n",
      "             Mlp-224             [-1, 197, 768]               0\n",
      "        Identity-225             [-1, 197, 768]               0\n",
      "        Identity-226             [-1, 197, 768]               0\n",
      "           Block-227             [-1, 197, 768]               0\n",
      "       LayerNorm-228             [-1, 197, 768]           1,536\n",
      "          Linear-229            [-1, 197, 2304]       1,771,776\n",
      "        Identity-230          [-1, 12, 197, 64]               0\n",
      "        Identity-231          [-1, 12, 197, 64]               0\n",
      "          Linear-232             [-1, 197, 768]         590,592\n",
      "         Dropout-233             [-1, 197, 768]               0\n",
      "       Attention-234             [-1, 197, 768]               0\n",
      "        Identity-235             [-1, 197, 768]               0\n",
      "        Identity-236             [-1, 197, 768]               0\n",
      "       LayerNorm-237             [-1, 197, 768]           1,536\n",
      "          Linear-238            [-1, 197, 3072]       2,362,368\n",
      "            GELU-239            [-1, 197, 3072]               0\n",
      "         Dropout-240            [-1, 197, 3072]               0\n",
      "        Identity-241            [-1, 197, 3072]               0\n",
      "          Linear-242             [-1, 197, 768]       2,360,064\n",
      "         Dropout-243             [-1, 197, 768]               0\n",
      "             Mlp-244             [-1, 197, 768]               0\n",
      "        Identity-245             [-1, 197, 768]               0\n",
      "        Identity-246             [-1, 197, 768]               0\n",
      "           Block-247             [-1, 197, 768]               0\n",
      "       LayerNorm-248             [-1, 197, 768]           1,536\n",
      "        Identity-249                  [-1, 768]               0\n",
      "         Dropout-250                  [-1, 768]               0\n",
      "          Linear-251                   [-1, 38]          29,222\n",
      "VisionTransformer-252                   [-1, 38]               0\n",
      "================================================================\n",
      "Total params: 85,675,820\n",
      "Trainable params: 85,675,820\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 480.18\n",
      "Params size (MB): 326.83\n",
      "Estimated Total Size (MB): 807.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "#net = ResNet101Modified(num_classes=5)\n",
    "net =vit_model(num_classes=38)\n",
    "summary(net.to('cuda'), (1,224,224))\n",
    "\n",
    "\n",
    "#================================================================\n",
    "#Total params: 42,510,405\n",
    "#Trainable params: 42,510,405\n",
    "#Non-trainable params: 0\n",
    "#----------------------------------------------------------------\n",
    "#Input size (MB): 0.57\n",
    "#Forward/backward pass size (MB): 429.72\n",
    "#Params size (MB): 162.16\n",
    "#Estimated Total Size (MB): 592.46\n",
    "#----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T05:56:58.708007Z",
     "iopub.status.busy": "2024-12-13T05:56:58.707548Z",
     "iopub.status.idle": "2024-12-13T06:10:03.246990Z",
     "shell.execute_reply": "2024-12-13T06:10:03.245713Z",
     "shell.execute_reply.started": "2024-12-13T05:56:58.707958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6a76c423b04fed92956e3a8580d45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/171M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch[1/10] loss:2.236: 100%|███████████████████████████████| 476/476 [00:52<00:00,  9.08it/s]\n",
      "[epoch 1] train_loss: 2.236 train_acc: 0.489 val_accuracy: 0.790\n",
      "train epoch[2/10] loss:1.675: 100%|███████████████████████████████| 476/476 [00:48<00:00,  9.89it/s]\n",
      "[epoch 2] train_loss: 1.675 train_acc: 0.846 val_accuracy: 0.891\n",
      "train epoch[3/10] loss:0.431: 100%|███████████████████████████████| 476/476 [00:48<00:00,  9.90it/s]\n",
      "[epoch 3] train_loss: 0.431 train_acc: 0.913 val_accuracy: 0.903\n",
      "train epoch[4/10] loss:0.609: 100%|███████████████████████████████| 476/476 [00:47<00:00,  9.99it/s]\n",
      "[epoch 4] train_loss: 0.609 train_acc: 0.939 val_accuracy: 0.922\n",
      "train epoch[5/10] loss:0.206: 100%|███████████████████████████████| 476/476 [00:47<00:00, 10.03it/s]\n",
      "[epoch 5] train_loss: 0.206 train_acc: 0.959 val_accuracy: 0.925\n",
      "train epoch[6/10] loss:0.398: 100%|███████████████████████████████| 476/476 [00:47<00:00, 10.00it/s]\n",
      "[epoch 6] train_loss: 0.398 train_acc: 0.968 val_accuracy: 0.918\n",
      "train epoch[7/10] loss:0.481: 100%|███████████████████████████████| 476/476 [00:47<00:00, 10.02it/s]\n",
      "[epoch 7] train_loss: 0.481 train_acc: 0.976 val_accuracy: 0.927\n",
      "train epoch[8/10] loss:0.119: 100%|███████████████████████████████| 476/476 [00:47<00:00,  9.99it/s]\n",
      "[epoch 8] train_loss: 0.119 train_acc: 0.980 val_accuracy: 0.934\n",
      "train epoch[9/10] loss:0.521: 100%|███████████████████████████████| 476/476 [00:47<00:00,  9.95it/s]\n",
      "[epoch 9] train_loss: 0.521 train_acc: 0.985 val_accuracy: 0.926\n",
      "train epoch[10/10] loss:0.174: 100%|██████████████████████████████| 476/476 [00:47<00:00,  9.95it/s]\n",
      "[epoch 10] train_loss: 0.174 train_acc: 0.983 val_accuracy: 0.935\n",
      "Finished Training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27/3088243163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mconf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Normalize the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_labels' is not defined"
     ]
    }
   ],
   "source": [
    "## Without Confusion Matrix \n",
    "# %%time\n",
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "#Select the Model to train \n",
    "#net = VGG(num_classes=38)\n",
    "#net = inceptionnext(num_classes=38)\n",
    "#net = convnext(num_classes=38)\n",
    "net = resnet50(num_classes=38)\n",
    "#net = alexnet(num_classes=38)\n",
    "net.to(device)\n",
    "# print(summary(net, (1,52,52)))\n",
    "\n",
    "loss_function = CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=3e-5)\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "    ############################train训练#############################################\n",
    "    net.train() # 声明正在训练\n",
    "    acc_num = torch.zeros(1).to(device) # 初始化，用于计算训练过程中预测正确的数量\n",
    "    sample_num = 0 # 初始化，用于记录当前迭代中，已经计算了多少个样本\n",
    "    train_bar = tqdm(train_loader, file=sys.stdout, ncols=100)\n",
    "    for step, data in enumerate(train_bar):\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0] # 【batch, channel, height, width】\n",
    "        optimizer.zero_grad()\n",
    "#         print(images.to(device).device)\n",
    "        images = images.float()\n",
    "#。     print(images.shape)\n",
    "        outputs = net(images.to(device)) # 输出维度：[batch_size, num_classes]，且输入需要是一个张量，输出是个张量\n",
    "        pred_class = torch.max(outputs, dim=1)[1]  # torch.max返回一个tuple，第一个是最大的值，第二个是最大的值的索引\n",
    "        acc_num += torch.eq(pred_class, labels.to(device)).sum() # 这一批数据有多少个预测正确的个数加到acc_num\n",
    "        \n",
    "        loss = loss_function(outputs, labels.to(device)) # 输出的也是张量，但是label不是\n",
    "        loss.backward() # 自动求导\n",
    "        optimizer.step() # 梯度下降\n",
    "        \n",
    "        train_acc = acc_num.item() / sample_num\n",
    "        train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch+1, epochs, loss) # 一批训练完就更新准确率\n",
    "     ########################validate验证############################################\n",
    "    net.eval() # 固定模型中所有参数\n",
    "    acc_num = 0.0\n",
    "    with torch.no_grad(): # 固定梯度\n",
    "        for val_data in test_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images = val_images.float()\n",
    "            outputs = net(val_images.to(device))\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            acc_num += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "    val_accurate = acc_num / val_num\n",
    "    print('[epoch %d] train_loss: %.3f train_acc: %.3f val_accuracy: %.3f' % (epoch+1, loss, train_acc, val_accurate))\n",
    "    \n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        \n",
    "    train_acc = 0.0 # 清空\n",
    "    val_accurate = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Results with the SOTA on the classification task \n",
    "\n",
    "ViT Model\n",
    "train_loss: 0.000 train_acc: 0.997 val_accuracy: 0.982\n",
    "\n",
    "InceptionNext \n",
    "train_loss: 1.238 train_acc: 0.590 val_accuracy: 0.592\n",
    "\n",
    "convnext\n",
    "train_loss: 0.003 train_acc: 0.984 val_accuracy: 0.895\n",
    "\n",
    "ResNet 152 \n",
    " train_loss: 0.019 train_acc: 0.996 val_accuracy: 0.953\n",
    "\n",
    "ResNet 101 \n",
    " train_loss: 0.058 train_acc: 0.995 val_accuracy: 0.954\n",
    "\n",
    "ResNet 50 \n",
    " train_loss: 0.003 train_acc: 0.978 val_accuracy: 0.966\n",
    "\n",
    "ResNet18 \n",
    " train_loss: 0.057 train_acc: 0.994 val_accuracy: 0.944\n",
    "\n",
    "AlexNet \n",
    " train_loss: 0.006 train_acc: 0.982 val_accuracy: 0.969\n",
    "\n",
    "Efficient_B0\n",
    "train_loss: 0.761 train_acc: 0.963 val_accuracy: 0.882\n",
    "\n",
    "EfficientNet_B7\n",
    "train_loss: 0.631 train_acc: 0.966 val_accuracy: 0.909\n",
    "\n",
    "MobileNet_v3\n",
    "train_loss: 0.092 train_acc: 0.934 val_accuracy: 0.825\n",
    "\n",
    "MobileNet_v3_large\n",
    "train_loss: 0.971 train_acc: 0.977 val_accuracy: 0.741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:42:51.931286Z",
     "iopub.status.busy": "2025-01-24T02:42:51.930500Z",
     "iopub.status.idle": "2025-01-24T02:42:52.054544Z",
     "shell.execute_reply": "2025-01-24T02:42:52.053513Z",
     "shell.execute_reply.started": "2025-01-24T02:42:51.931242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 38])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Create dummy grayscale data as a NumPy array\n",
    "images_np = np.random.randn(64, 1, 52, 52)  # Example batch of grayscale images\n",
    "\n",
    "# Convert NumPy array to PyTorch tensor\n",
    "images = torch.tensor(images_np, dtype=torch.float32)\n",
    "\n",
    "# Duplicate the single grayscale channel to create a pseudo-RGB image\n",
    "images = images.repeat(1, 3, 1, 1)  # Duplicate along the channel dimension\n",
    "\n",
    "# Define preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 (expected by CLIP and similar models)\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),  # CLIP normalization mean\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711))   # CLIP normalization std\n",
    "])\n",
    "\n",
    "# Apply preprocessing to all images in the batch\n",
    "images = torch.stack([preprocess(image) for image in images])\n",
    "\n",
    "# Move images to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images = images.to(device)\n",
    "\n",
    "# Assume `net` is your model\n",
    "net.to(device)\n",
    "\n",
    "# Perform a forward pass\n",
    "outputs = net(images)\n",
    "print(outputs.shape)  # Expected: [64, num_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:42:58.026030Z",
     "iopub.status.busy": "2025-01-24T02:42:58.025038Z",
     "iopub.status.idle": "2025-01-24T02:42:58.032683Z",
     "shell.execute_reply": "2025-01-24T02:42:58.031724Z",
     "shell.execute_reply.started": "2025-01-24T02:42:58.025990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        # Load the CLIP model\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Modify the classifier head\n",
    "        self.classifier = nn.Linear(self.clip_model.visual.output_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Use the visual encoder of CLIP\n",
    "        visual_features = self.clip_model.encode_image(x)\n",
    "        \n",
    "        # Ensure visual_features has the same dtype as classifier weights\n",
    "        visual_features = visual_features.to(self.classifier.weight.dtype)\n",
    "        \n",
    "        output = self.classifier(visual_features)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "def clip_model(num_classes):\n",
    "    model = CustomCLIPModel(num_classes=num_classes)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T07:03:10.596145Z",
     "iopub.status.busy": "2025-02-06T07:03:10.594761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 6.92 GiB is free. Of the allocated memory 295.00 KiB is allocated by PyTorch, and 1.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 32\u001b[0m\n\u001b[0;32m     16\u001b[0m net \u001b[38;5;241m=\u001b[39m VGG(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m38\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#net = inceptionnext(num_classes=38)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#net = convnext(num_classes=38)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#net = resnet101(num_classes=38)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#net = mobilenet_v3_small_single_channel(num_classes=38)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#net = mobilenet_v3_large_single_channel(num_classes=38)\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Loss and optimizer\u001b[39;00m\n\u001b[0;32m     34\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[1;32mc:\\Users\\kim\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\kim\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kim\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kim\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kim\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1327\u001b[0m         device,\n\u001b[0;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m         non_blocking,\n\u001b[0;32m   1330\u001b[0m     )\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 6.92 GiB is free. Of the allocated memory 295.00 KiB is allocated by PyTorch, and 1.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#import clip\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume `net` is your model, already initialized and moved to `device`\n",
    "# Assume `train_loader` and `test_loader` are your DataLoader objects\n",
    "\n",
    "# device = \"cpu\"\n",
    "#Select the Model to train \n",
    "net = VGG(num_classes=38)\n",
    "#net = inceptionnext(num_classes=38)\n",
    "#net = convnext(num_classes=38)\n",
    "#net = resnet101(num_classes=38)\n",
    "#net = alexnet(num_classes=38)\n",
    "#net = efficientnet_b7(num_classes=38)\n",
    "#net = clip_model(num_classes=38)\n",
    "#net = blip(num_classes=38)\n",
    "#net = mobilenet_V2(num_classes=38)\n",
    "#net = transformer_net(num_classes=38)\n",
    "#net = vit_model(num_classes = 38)\n",
    "#net = few_shot_vit_model(n_support=5, n_query=10, num_classes=5)\n",
    "#net = mobilenet_v3_small_single_channel(num_classes=38)\n",
    "#net = mobilenet_v3_large_single_channel(num_classes=38)\n",
    "\n",
    "\n",
    "net.to(device)\n",
    "# Loss and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=3e-5)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 30\n",
    "best_acc = 0.0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    acc_num = torch.zeros(1).to(device)\n",
    "    sample_num = 0\n",
    "    train_bar = tqdm(train_loader, file=sys.stdout, ncols=100)\n",
    "\n",
    "    for step, data in enumerate(train_bar):\n",
    "        images, labels = data\n",
    "        images, labels = images.float().to(device), labels.to(device)\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        pred_class = torch.max(outputs, dim=1)[1]\n",
    "        acc_num += torch.eq(pred_class, labels).sum()\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc = acc_num.item() / sample_num\n",
    "        train_bar.desc = f\"train epoch[{epoch + 1}/{epochs}] loss:{loss:.3f}\"\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    acc_num = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_data in test_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            val_images, val_labels = val_images.float().to(device), val_labels.to(device)\n",
    "            outputs = net(val_images)\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "\n",
    "            acc_num += torch.eq(predict_y, val_labels).sum().item()\n",
    "            all_labels.extend(val_labels.cpu().numpy())\n",
    "            all_preds.extend(predict_y.cpu().numpy())\n",
    "\n",
    "    val_accurate = acc_num / len(test_loader.dataset)\n",
    "    print(f\"[epoch {epoch + 1}] train_loss: {loss:.3f} train_acc: {train_acc:.3f} val_accuracy: {val_accurate:.3f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        torch.save(net.state_dict(), \"best_model.pth\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap=\"Blues\", xticklabels=range(len(conf_matrix)),\n",
    "            yticklabels=range(len(conf_matrix)))\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:09:48.906686Z",
     "iopub.status.busy": "2025-01-24T03:09:48.906066Z",
     "iopub.status.idle": "2025-01-24T03:10:27.370500Z",
     "shell.execute_reply": "2025-01-24T03:10:27.369426Z",
     "shell.execute_reply.started": "2025-01-24T03:09:48.906628Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 32.3050, Accuracy: 11.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models import create_model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FewShotViTModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, img_size=224):\n",
    "        super(FewShotViTModel, self).__init__()\n",
    "        # Convert grayscale to 3-channel\n",
    "        self.input_adapter = nn.Conv2d(1, 3, kernel_size=1)  # Converts [1, H, W] to [3, H, W]\n",
    "        \n",
    "        # Vision Transformer backbone\n",
    "        self.backbone = create_model(\n",
    "            'vit_base_patch16_224',  # Pre-trained ViT model from timm\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # Exclude the classifier layer\n",
    "            img_size=img_size,\n",
    "            in_chans=3,  # Input channels (after adapter)\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.feature_dim = self.backbone.embed_dim  # Output dimension of the ViT feature extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Resize input to match model requirements\n",
    "        x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)\n",
    "        x = self.input_adapter(x)  # Convert grayscale to RGB\n",
    "        features = self.backbone(x)  # Extract features using ViT\n",
    "        return features\n",
    "\n",
    "class FewShotLearner:\n",
    "    def __init__(self, model, n_support, n_query):\n",
    "        \"\"\"\n",
    "        Initialize the few-shot learner.\n",
    "\n",
    "        Args:\n",
    "        - model: Feature extractor (e.g., ViTModel).\n",
    "        - n_support: Number of support samples per class.\n",
    "        - n_query: Number of query samples per class.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "\n",
    "    def compute_prototypes(self, support_features, support_labels):\n",
    "        \"\"\"\n",
    "        Compute class prototypes from support set.\n",
    "\n",
    "        Args:\n",
    "        - support_features: Tensor of shape [n_classes * n_support, feature_dim].\n",
    "        - support_labels: Tensor of shape [n_classes * n_support].\n",
    "\n",
    "        Returns:\n",
    "        - prototypes: Tensor of shape [n_classes, feature_dim].\n",
    "        \"\"\"\n",
    "        prototypes = []\n",
    "        n_classes = support_labels.unique().size(0)\n",
    "        for cls in range(n_classes):\n",
    "            cls_features = support_features[support_labels == cls]\n",
    "            prototype = cls_features.mean(dim=0)  # Compute mean feature for each class\n",
    "            prototypes.append(prototype)\n",
    "        return torch.stack(prototypes, dim=0)\n",
    "\n",
    "    def classify(self, query_features, prototypes):\n",
    "        \"\"\"\n",
    "        Classify query samples based on prototypes.\n",
    "\n",
    "        Args:\n",
    "        - query_features: Tensor of shape [n_classes * n_query, feature_dim].\n",
    "        - prototypes: Tensor of shape [n_classes, feature_dim].\n",
    "\n",
    "        Returns:\n",
    "        - logits: Tensor of shape [n_classes * n_query, n_classes].\n",
    "        \"\"\"\n",
    "        # Compute squared Euclidean distance between query features and prototypes\n",
    "        distances = torch.cdist(query_features, prototypes, p=2) ** 2\n",
    "        logits = -distances  # Convert distances to logits (negative for classification)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, support_set, support_labels, query_set, query_labels):\n",
    "        \"\"\"\n",
    "        Perform few-shot learning.\n",
    "\n",
    "        Args:\n",
    "        - support_set: Tensor of shape [n_classes * n_support, 1, H, W].\n",
    "        - support_labels: Tensor of shape [n_classes * n_support].\n",
    "        - query_set: Tensor of shape [n_classes * n_query, 1, H, W].\n",
    "        - query_labels: Tensor of shape [n_classes * n_query].\n",
    "\n",
    "        Returns:\n",
    "        - loss: Few-shot classification loss.\n",
    "        - accuracy: Few-shot classification accuracy.\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        support_features = self.model(support_set)\n",
    "        query_features = self.model(query_set)\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = self.compute_prototypes(support_features, support_labels)\n",
    "\n",
    "        # Classify query set\n",
    "        logits = self.classify(query_features, prototypes)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, query_labels)\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predictions = logits.max(dim=1)\n",
    "        accuracy = (predictions == query_labels).float().mean()\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "# Instantiate the model and few-shot learner\n",
    "def few_shot_vit_model(n_support=5, n_query=15, num_classes=5):\n",
    "    model = FewShotViTModel(num_classes=num_classes)\n",
    "    learner = FewShotLearner(model, n_support=n_support, n_query=n_query)\n",
    "    return learner\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dummy support and query sets\n",
    "    n_classes = 10\n",
    "    n_support = 5\n",
    "    n_query = 10\n",
    "    img_size = 52\n",
    "\n",
    "    learner = few_shot_vit_model(n_support=n_support, n_query=n_query, num_classes=n_classes)\n",
    "\n",
    "    support_set = torch.randn(n_classes * n_support, 1, img_size, img_size)\n",
    "    support_labels = torch.arange(n_classes).repeat_interleave(n_support)\n",
    "    query_set = torch.randn(n_classes * n_query, 1, img_size, img_size)\n",
    "    query_labels = torch.arange(n_classes).repeat_interleave(n_query)\n",
    "\n",
    "    # Forward pass\n",
    "    loss, accuracy = learner.forward(support_set, support_labels, query_set, query_labels)\n",
    "    print(f\"Loss: {loss.item():.4f}, Accuracy: {accuracy.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-24T02:25:57.127Z",
     "iopub.execute_input": "2025-01-24T02:25:10.640313Z",
     "iopub.status.busy": "2025-01-24T02:25:10.639890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the few-shot learning parameters\n",
    "    n_classes = 11\n",
    "    n_support = 15\n",
    "    n_query = 11\n",
    "    img_size = 52\n",
    "\n",
    "    # Instantiate the model and few-shot learner\n",
    "    learner = few_shot_vit_model(n_support=n_support, n_query=n_query, num_classes=n_classes)\n",
    "\n",
    "    # Create dummy support and query sets\n",
    "    support_set = torch.randn(n_classes * n_support, 1, img_size, img_size)\n",
    "    support_labels = torch.arange(n_classes).repeat_interleave(n_support)\n",
    "    query_set = torch.randn(n_classes * n_query, 1, img_size, img_size)\n",
    "    query_labels = torch.arange(n_classes).repeat_interleave(n_query)\n",
    "\n",
    "    # Loss function\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (Use parameters from the underlying ViT model)\n",
    "    optimizer = torch.optim.Adam(learner.model.parameters(), lr=3e-5)\n",
    "\n",
    "    # Forward pass\n",
    "    loss, accuracy = learner.forward(support_set, support_labels, query_set, query_labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item():.4f}, Accuracy: {accuracy.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-24T03:11:18.261Z",
     "iopub.execute_input": "2025-01-24T03:11:01.416804Z",
     "iopub.status.busy": "2025-01-24T03:11:01.415763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training loop with evaluation metrics\n",
    "def train_few_shot_model(learner, support_set, support_labels, query_set, query_labels, num_epochs=10, lr=3e-5):\n",
    "    # Loss and optimizer\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(learner.model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = learner.forward(support_set, support_labels, query_set, query_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on query set\n",
    "        learner.model.eval()\n",
    "        with torch.no_grad():\n",
    "            query_logits = learner.model(query_set)\n",
    "            query_preds = F.softmax(query_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(query_labels.cpu(), query_preds.cpu())\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            query_labels.cpu(), query_preds.cpu(), average=None, labels=torch.unique(query_labels).cpu()\n",
    "        )\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"[Epoch {epoch}]\")\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        for i, cls in enumerate(torch.unique(query_labels).cpu().numpy()):\n",
    "            print(f\"Class {cls}: Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1 Score: {f1[i]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        learner.model.train()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the few-shot learning parameters\n",
    "    n_classes = 30\n",
    "    n_support = 500\n",
    "    n_query = 15\n",
    "    img_size = 52\n",
    "    num_epochs = 30  # Vary the number of epochs here\n",
    "\n",
    "    # Instantiate the model and few-shot learner\n",
    "    learner = few_shot_vit_model(n_support=n_support, n_query=n_query, num_classes=n_classes)\n",
    "\n",
    "    # Create dummy support and query sets\n",
    "    support_set = torch.randn(n_classes * n_support, 1, img_size, img_size)\n",
    "    support_labels = torch.arange(n_classes).repeat_interleave(n_support)\n",
    "    query_set = torch.randn(n_classes * n_query, 1, img_size, img_size)\n",
    "    query_labels = torch.arange(n_classes).repeat_interleave(n_query)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    train_few_shot_model(\n",
    "        learner, support_set, support_labels, query_set, query_labels, num_epochs=num_epochs, lr=3e-5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "InceptionNext \n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       199\n",
    "           1     0.9052    0.9550    0.9294       200\n",
    "           2     0.6786    0.7238    0.7005       210\n",
    "           3     0.6531    0.7767    0.7095       206\n",
    "           4     0.6792    0.8534    0.7564       191\n",
    "           5     0.6500    0.6566    0.6533       198\n",
    "           6     0.9032    0.9032    0.9032        31\n",
    "           7     0.7980    0.8571    0.8265       189\n",
    "           8     0.9167    0.8902    0.9032       173\n",
    "           9     0.6402    0.5738    0.6052       183\n",
    "          10     0.5294    0.8844    0.6623       173\n",
    "          11     0.6457    0.5355    0.5855       211\n",
    "          12     0.7542    0.8683    0.8073       205\n",
    "          13     0.4869    0.4581    0.4721       203\n",
    "          14     0.3807    0.8152    0.5190       184\n",
    "          15     0.5429    0.1900    0.2815       200\n",
    "          16     0.5628    0.6190    0.5896       210\n",
    "          17     0.7465    0.5436    0.6291       195\n",
    "          18     0.6611    0.5385    0.5935       221\n",
    "          19     0.4500    0.7844    0.5719       218\n",
    "          20     0.6667    0.6597    0.6632       191\n",
    "          21     0.5822    0.7294    0.6475       170\n",
    "          22     0.5411    0.3780    0.4451       209\n",
    "          23     0.6334    0.7530    0.6881       413\n",
    "          24     0.4392    0.4716    0.4548       176\n",
    "          25     0.6193    0.5317    0.5722       205\n",
    "          26     0.6136    0.6618    0.6368       204\n",
    "          27     0.6242    0.5052    0.5584       194\n",
    "          28     0.4734    0.3524    0.4040       227\n",
    "          29     0.5000    0.2362    0.3208       199\n",
    "          30     0.4244    0.3782    0.4000       193\n",
    "          31     0.4744    0.7475    0.5804       198\n",
    "          32     0.7013    0.5870    0.6391       184\n",
    "          33     0.7115    0.1602    0.2615       231\n",
    "          34     0.5403    0.3472    0.4227       193\n",
    "          35     0.6111    0.4692    0.5308       211\n",
    "          36     0.5410    0.6804    0.6027       194\n",
    "          37     0.7711    0.6066    0.6790       211\n",
    "\n",
    "    accuracy                         0.6154      7603\n",
    "   macro avg     0.6330    0.6232    0.6107      7603\n",
    "weighted avg     0.6269    0.6154    0.6031      7603\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ViT \n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       196\n",
    "           1     0.9909    0.9954    0.9932       219\n",
    "           2     0.9950    0.9950    0.9950       199\n",
    "           3     1.0000    0.9756    0.9877       205\n",
    "           4     0.9690    0.9955    0.9821       220\n",
    "           5     0.9951    0.9855    0.9903       207\n",
    "           6     0.9375    1.0000    0.9677        30\n",
    "           7     0.9896    1.0000    0.9948       190\n",
    "           8     1.0000    0.9881    0.9940       168\n",
    "           9     0.9764    0.9673    0.9718       214\n",
    "          10     0.9907    0.9727    0.9817       220\n",
    "          11     0.9713    0.9951    0.9831       204\n",
    "          12     0.9724    0.9679    0.9701       218\n",
    "          13     0.9948    0.9697    0.9821       198\n",
    "          14     0.9944    0.9890    0.9917       181\n",
    "          15     0.9752    0.9949    0.9850       198\n",
    "          16     0.9784    1.0000    0.9891       181\n",
    "          17     0.9955    1.0000    0.9977       219\n",
    "          18     1.0000    0.9848    0.9923       197\n",
    "          19     0.9707    0.9755    0.9731       204\n",
    "          20     0.9808    0.9903    0.9855       206\n",
    "          21     0.9757    0.9901    0.9829       203\n",
    "          22     0.9567    0.9803    0.9684       203\n",
    "          23     0.9949    0.9774    0.9861       399\n",
    "          24     0.9487    0.9893    0.9686       187\n",
    "          25     0.9831    1.0000    0.9915       175\n",
    "          26     0.9467    0.9357    0.9412       171\n",
    "          27     0.9722    0.9887    0.9804       177\n",
    "          28     0.9806    0.9758    0.9782       207\n",
    "          29     0.9531    1.0000    0.9760       183\n",
    "          30     0.9710    1.0000    0.9853       201\n",
    "          31     0.9848    0.9750    0.9799       200\n",
    "          32     1.0000    0.9724    0.9860       217\n",
    "          33     0.9657    0.9704    0.9681       203\n",
    "          34     0.9572    0.9521    0.9547       188\n",
    "          35     0.9950    0.9709    0.9828       206\n",
    "          36     1.0000    0.9482    0.9734       193\n",
    "          37     0.9856    0.9537    0.9694       216\n",
    "\n",
    "    accuracy                         0.9816      7603\n",
    "   macro avg     0.9802    0.9822    0.9811      7603\n",
    "weighted avg     0.9818    0.9816    0.9816      7603\n",
    "\n",
    "\n",
    "EfficientNet_B0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       197\n",
    "           1     0.9248    0.9766    0.9500       214\n",
    "           2     0.8558    0.9735    0.9109       189\n",
    "           3     0.9420    0.9548    0.9483       221\n",
    "           4     0.9116    0.9655    0.9378       203\n",
    "           5     0.9196    0.8472    0.8819       216\n",
    "           6     0.9167    0.9706    0.9429        34\n",
    "           7     0.9200    0.9952    0.9561       208\n",
    "           8     0.9941    0.9655    0.9796       174\n",
    "           9     0.9388    0.8976    0.9177       205\n",
    "          10     0.8200    0.9903    0.8972       207\n",
    "          11     0.8756    0.8756    0.8756       201\n",
    "          12     0.9021    0.9815    0.9401       216\n",
    "          13     0.7608    0.8503    0.8030       187\n",
    "          14     0.8465    0.9146    0.8792       199\n",
    "          15     0.8476    0.7354    0.7875       189\n",
    "          16     0.8163    0.9479    0.8772       211\n",
    "          17     0.9204    0.9391    0.9296       197\n",
    "          18     0.9100    0.9014    0.9057       213\n",
    "          19     0.8907    0.8670    0.8787       188\n",
    "          20     0.8644    0.9401    0.9007       217\n",
    "          21     0.8436    0.9223    0.8812       193\n",
    "          22     0.8182    0.8308    0.8244       195\n",
    "          23     0.9442    0.9690    0.9564       419\n",
    "          24     0.8246    0.7540    0.7877       187\n",
    "          25     0.8984    0.8889    0.8936       189\n",
    "          26     0.9026    0.8627    0.8822       204\n",
    "          27     0.9714    0.8543    0.9091       199\n",
    "          28     0.8081    0.7583    0.7824       211\n",
    "          29     0.9409    0.9358    0.9383       187\n",
    "          30     0.7854    0.8269    0.8056       208\n",
    "          31     0.7790    0.8011    0.7899       176\n",
    "          32     0.9427    0.7708    0.8481       192\n",
    "          33     0.8743    0.8520    0.8630       196\n",
    "          34     0.9007    0.7056    0.7913       180\n",
    "          35     0.8662    0.7861    0.8242       173\n",
    "          36     0.9221    0.7396    0.8208       192\n",
    "          37     0.8950    0.8287    0.8606       216\n",
    "\n",
    "    accuracy                         0.8861      7603\n",
    "   macro avg     0.8867    0.8836    0.8831      7603\n",
    "weighted avg     0.8880    0.8861    0.8851      7603\n",
    "\n",
    "\n",
    "EfficientNet_B7\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9949    1.0000    0.9975       197\n",
    "           1     0.9264    1.0000    0.9618       214\n",
    "           2     0.8832    1.0000    0.9380       189\n",
    "           3     0.9241    0.9367    0.9303       221\n",
    "           4     0.9132    0.9852    0.9479       203\n",
    "           5     0.8642    0.9722    0.9150       216\n",
    "           6     1.0000    0.8235    0.9032        34\n",
    "           7     0.9367    0.9952    0.9650       208\n",
    "           8     0.9663    0.9885    0.9773       174\n",
    "           9     0.9505    0.9366    0.9435       205\n",
    "          10     0.8922    1.0000    0.9431       207\n",
    "          11     0.8468    0.9353    0.8889       201\n",
    "          12     0.9181    0.9861    0.9509       216\n",
    "          13     0.9364    0.8663    0.9000       187\n",
    "          14     0.8905    0.9397    0.9144       199\n",
    "          15     0.8652    0.8148    0.8392       189\n",
    "          16     0.8432    0.9431    0.8904       211\n",
    "          17     0.8990    0.9492    0.9235       197\n",
    "          18     0.9590    0.8779    0.9167       213\n",
    "          19     0.8995    0.9043    0.9019       188\n",
    "          20     0.9298    0.9770    0.9528       217\n",
    "          21     0.9412    0.8290    0.8815       193\n",
    "          22     0.8907    0.8359    0.8624       195\n",
    "          23     0.9757    0.9594    0.9675       419\n",
    "          24     0.8548    0.8503    0.8525       187\n",
    "          25     0.9133    0.9471    0.9299       189\n",
    "          26     0.9167    0.8088    0.8594       204\n",
    "          27     0.9786    0.9196    0.9482       199\n",
    "          28     0.8356    0.8673    0.8512       211\n",
    "          29     0.9405    0.9305    0.9355       187\n",
    "          30     0.8043    0.9087    0.8533       208\n",
    "          31     0.8210    0.7557    0.7870       176\n",
    "          32     0.9682    0.7917    0.8711       192\n",
    "          33     0.9110    0.8878    0.8992       196\n",
    "          34     0.9643    0.7500    0.8437       180\n",
    "          35     0.8686    0.8786    0.8736       173\n",
    "          36     0.9856    0.7135    0.8278       192\n",
    "          37     0.8533    0.8889    0.8707       216\n",
    "\n",
    "    accuracy                         0.9094      7603\n",
    "   macro avg     0.9122    0.9041    0.9057      7603\n",
    "weighted avg     0.9118    0.9094    0.9084      7603\n",
    "\n",
    "MobileNet_v2 \n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9220    0.9793    0.9497       193\n",
    "           1     0.3739    0.4195    0.3954       205\n",
    "           2     0.3298    0.3246    0.3272       191\n",
    "           3     0.3698    0.3679    0.3688       193\n",
    "           4     0.3906    0.4573    0.4213       199\n",
    "           5     0.2194    0.2667    0.2407       195\n",
    "           6     0.9444    0.9189    0.9315        37\n",
    "           7     0.3640    0.4505    0.4027       202\n",
    "           8     0.8400    0.7159    0.7730       176\n",
    "           9     0.2727    0.3015    0.2864       199\n",
    "          10     0.3486    0.3800    0.3636       200\n",
    "          11     0.1415    0.1667    0.1531       180\n",
    "          12     0.2609    0.2621    0.2615       206\n",
    "          13     0.2679    0.2333    0.2494       240\n",
    "          14     0.2562    0.2081    0.2297       197\n",
    "          15     0.2208    0.2684    0.2423       190\n",
    "          16     0.2657    0.2171    0.2390       175\n",
    "          17     0.2584    0.2727    0.2654       198\n",
    "          18     0.1872    0.2088    0.1974       182\n",
    "          19     0.2470    0.2864    0.2652       213\n",
    "          20     0.2515    0.2069    0.2270       203\n",
    "          21     0.2925    0.3163    0.3039       196\n",
    "          22     0.1493    0.1675    0.1579       197\n",
    "          23     0.6628    0.8593    0.7484       398\n",
    "          24     0.1934    0.2047    0.1989       171\n",
    "          25     0.2010    0.1872    0.1939       219\n",
    "          26     0.2830    0.3077    0.2948       195\n",
    "          27     0.2465    0.1832    0.2102       191\n",
    "          28     0.2191    0.1875    0.2021       208\n",
    "          29     0.3538    0.3538    0.3538       212\n",
    "          30     0.2418    0.1789    0.2056       246\n",
    "          31     0.2616    0.2206    0.2394       204\n",
    "          32     0.2353    0.2041    0.2186       196\n",
    "          33     0.3416    0.2594    0.2949       212\n",
    "          34     0.2182    0.1818    0.1983       198\n",
    "          35     0.3185    0.2358    0.2710       212\n",
    "          36     0.2450    0.2722    0.2579       180\n",
    "          37     0.3333    0.2938    0.3123       194\n",
    "\n",
    "    accuracy                         0.3289      7603\n",
    "   macro avg     0.3297    0.3296    0.3277      7603\n",
    "weighted avg     0.3239    0.3289    0.3241      7603\n",
    "\n",
    "MobileNet_v3\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9947    1.0000    0.9973       187\n",
    "           1     0.8806    0.9465    0.9124       187\n",
    "           2     0.8455    0.8995    0.8717       219\n",
    "           3     0.8595    0.8548    0.8571       186\n",
    "           4     0.8915    0.9220    0.9065       205\n",
    "           5     0.7864    0.8872    0.8337       195\n",
    "           6     0.9688    0.9394    0.9538        33\n",
    "           7     0.8899    0.9510    0.9194       204\n",
    "           8     0.9774    0.9665    0.9719       179\n",
    "           9     0.8232    0.8030    0.8130       203\n",
    "          10     0.8278    0.8238    0.8258       210\n",
    "          11     0.7857    0.7933    0.7895       208\n",
    "          12     0.8519    0.9154    0.8825       201\n",
    "          13     0.7018    0.7688    0.7338       199\n",
    "          14     0.7864    0.8020    0.7941       202\n",
    "          15     0.7486    0.6931    0.7198       189\n",
    "          16     0.8000    0.8528    0.8256       197\n",
    "          17     0.8670    0.8069    0.8359       202\n",
    "          18     0.8647    0.8483    0.8565       211\n",
    "          19     0.8293    0.8416    0.8354       202\n",
    "          20     0.8685    0.8981    0.8831       206\n",
    "          21     0.7655    0.8737    0.8160       198\n",
    "          22     0.7680    0.7413    0.7544       201\n",
    "          23     0.8700    0.9700    0.9173       400\n",
    "          24     0.7316    0.7380    0.7348       229\n",
    "          25     0.8830    0.7511    0.8117       221\n",
    "          26     0.8142    0.7340    0.7720       203\n",
    "          27     0.7362    0.7317    0.7339       164\n",
    "          28     0.7861    0.7248    0.7542       218\n",
    "          29     0.8978    0.8068    0.8499       207\n",
    "          30     0.7561    0.7908    0.7731       196\n",
    "          31     0.7437    0.7957    0.7688       186\n",
    "          32     0.7969    0.7688    0.7826       199\n",
    "          33     0.8087    0.7590    0.7831       195\n",
    "          34     0.7811    0.7135    0.7458       185\n",
    "          35     0.7795    0.7488    0.7638       203\n",
    "          36     0.8385    0.6959    0.7606       194\n",
    "          37     0.8375    0.7486    0.7906       179\n",
    "\n",
    "    accuracy                         0.8249      7603\n",
    "   macro avg     0.8275    0.8239    0.8245      7603\n",
    "weighted avg     0.8254    0.8249    0.8239      7603\n",
    "\n",
    "\n",
    "MobileNet_v3_large \n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9947    1.0000    0.9973       187\n",
    "           1     0.8646    0.8877    0.8760       187\n",
    "           2     0.7351    0.8995    0.8090       219\n",
    "           3     0.8519    0.7419    0.7931       186\n",
    "           4     0.8173    0.8293    0.8232       205\n",
    "           5     0.7772    0.7333    0.7546       195\n",
    "           6     0.9118    0.9394    0.9254        33\n",
    "           7     0.8125    0.9559    0.8784       204\n",
    "           8     0.9657    0.9441    0.9548       179\n",
    "           9     0.8726    0.6749    0.7611       203\n",
    "          10     0.8201    0.7381    0.7769       210\n",
    "          11     0.7475    0.7115    0.7291       208\n",
    "          12     0.8037    0.8557    0.8289       201\n",
    "          13     0.6786    0.5729    0.6213       199\n",
    "          14     0.7348    0.6584    0.6945       202\n",
    "          15     0.6140    0.5556    0.5833       189\n",
    "          16     0.6498    0.7157    0.6812       197\n",
    "          17     0.8272    0.6634    0.7363       202\n",
    "          18     0.7467    0.8104    0.7773       211\n",
    "          19     0.6963    0.7376    0.7163       202\n",
    "          20     0.7635    0.7524    0.7579       206\n",
    "          21     0.7261    0.8434    0.7804       198\n",
    "          22     0.6983    0.6219    0.6579       201\n",
    "          23     0.8348    0.9600    0.8930       400\n",
    "          24     0.6535    0.6507    0.6521       229\n",
    "          25     0.7788    0.7647    0.7717       221\n",
    "          26     0.7436    0.7143    0.7286       203\n",
    "          27     0.5556    0.5488    0.5521       164\n",
    "          28     0.6417    0.5505    0.5926       218\n",
    "          29     0.6468    0.7874    0.7102       207\n",
    "          30     0.6497    0.5867    0.6166       196\n",
    "          31     0.6313    0.6075    0.6192       186\n",
    "          32     0.7059    0.7236    0.7146       199\n",
    "          33     0.6435    0.7590    0.6965       195\n",
    "          34     0.6481    0.5676    0.6052       185\n",
    "          35     0.6758    0.7291    0.7014       203\n",
    "          36     0.6768    0.5722    0.6201       194\n",
    "          37     0.6127    0.6983    0.6527       179\n",
    "\n",
    "    accuracy                         0.7406      7603\n",
    "   macro avg     0.7423    0.7385    0.7379      7603\n",
    "weighted avg     0.7413    0.7406    0.7383      7603\n",
    "\n",
    "ConvNext \n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    0.9849    0.9924       199\n",
    "           1     0.9146    0.9100    0.9123       200\n",
    "           2     0.9175    0.8476    0.8812       210\n",
    "           3     0.9179    0.9223    0.9201       206\n",
    "           4     0.9442    0.9738    0.9588       191\n",
    "           5     0.8394    0.9242    0.8798       198\n",
    "           6     0.9355    0.9355    0.9355        31\n",
    "           7     0.9179    0.9471    0.9323       189\n",
    "           8     0.9884    0.9884    0.9884       173\n",
    "           9     0.9268    0.8306    0.8761       183\n",
    "          10     0.9286    0.9769    0.9521       173\n",
    "          11     0.8772    0.9479    0.9112       211\n",
    "          12     0.9208    0.9073    0.9140       205\n",
    "          13     0.8693    0.8522    0.8607       203\n",
    "          14     0.8817    0.8913    0.8865       184\n",
    "          15     0.9149    0.8600    0.8866       200\n",
    "          16     0.8485    0.9333    0.8889       210\n",
    "          17     0.8696    0.8205    0.8443       195\n",
    "          18     0.9073    0.8416    0.8732       221\n",
    "          19     0.8973    0.9220    0.9095       218\n",
    "          20     0.8386    0.9791    0.9034       191\n",
    "          21     0.8295    0.8588    0.8439       170\n",
    "          22     0.9274    0.7943    0.8557       209\n",
    "          23     0.9375    0.9806    0.9586       413\n",
    "          24     0.9412    0.8182    0.8754       176\n",
    "          25     0.9286    0.9512    0.9398       205\n",
    "          26     0.9133    0.8775    0.8950       204\n",
    "          27     0.9833    0.9124    0.9465       194\n",
    "          28     0.8698    0.8238    0.8462       227\n",
    "          29     0.9444    0.9397    0.9421       199\n",
    "          30     0.8281    0.9482    0.8841       193\n",
    "          31     0.8550    0.8636    0.8593       198\n",
    "          32     0.8642    0.7609    0.8092       184\n",
    "          33     0.8800    0.8571    0.8684       231\n",
    "          34     0.8579    0.8446    0.8512       193\n",
    "          35     0.8283    0.9147    0.8694       211\n",
    "          36     0.8259    0.8557    0.8405       194\n",
    "          37     0.9029    0.8815    0.8921       211\n",
    "\n",
    "    accuracy                         0.8978      7603\n",
    "   macro avg     0.8993    0.8968    0.8970      7603\n",
    "weighted avg     0.8992    0.8978    0.8974      7603\n",
    "\n",
    "InceptionNext \n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       199\n",
    "           1     0.9052    0.9550    0.9294       200\n",
    "           2     0.6786    0.7238    0.7005       210\n",
    "           3     0.6531    0.7767    0.7095       206\n",
    "           4     0.6792    0.8534    0.7564       191\n",
    "           5     0.6500    0.6566    0.6533       198\n",
    "           6     0.9032    0.9032    0.9032        31\n",
    "           7     0.7980    0.8571    0.8265       189\n",
    "           8     0.9167    0.8902    0.9032       173\n",
    "           9     0.6402    0.5738    0.6052       183\n",
    "          10     0.5294    0.8844    0.6623       173\n",
    "          11     0.6457    0.5355    0.5855       211\n",
    "          12     0.7542    0.8683    0.8073       205\n",
    "          13     0.4869    0.4581    0.4721       203\n",
    "          14     0.3807    0.8152    0.5190       184\n",
    "          15     0.5429    0.1900    0.2815       200\n",
    "          16     0.5628    0.6190    0.5896       210\n",
    "          17     0.7465    0.5436    0.6291       195\n",
    "          18     0.6611    0.5385    0.5935       221\n",
    "          19     0.4500    0.7844    0.5719       218\n",
    "          20     0.6667    0.6597    0.6632       191\n",
    "          21     0.5822    0.7294    0.6475       170\n",
    "          22     0.5411    0.3780    0.4451       209\n",
    "          23     0.6334    0.7530    0.6881       413\n",
    "          24     0.4392    0.4716    0.4548       176\n",
    "          25     0.6193    0.5317    0.5722       205\n",
    "          26     0.6136    0.6618    0.6368       204\n",
    "          27     0.6242    0.5052    0.5584       194\n",
    "          28     0.4734    0.3524    0.4040       227\n",
    "          29     0.5000    0.2362    0.3208       199\n",
    "          30     0.4244    0.3782    0.4000       193\n",
    "          31     0.4744    0.7475    0.5804       198\n",
    "          32     0.7013    0.5870    0.6391       184\n",
    "          33     0.7115    0.1602    0.2615       231\n",
    "          34     0.5403    0.3472    0.4227       193\n",
    "          35     0.6111    0.4692    0.5308       211\n",
    "          36     0.5410    0.6804    0.6027       194\n",
    "          37     0.7711    0.6066    0.6790       211\n",
    "\n",
    "    accuracy                         0.6154      7603\n",
    "   macro avg     0.6330    0.6232    0.6107      7603\n",
    "weighted avg     0.6269    0.6154    0.6031      7603\n",
    "\n",
    "\n",
    "RESNET 101 \n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       227\n",
    "           1     0.9471    1.0000    0.9729       215\n",
    "           2     0.9453    0.9744    0.9596       195\n",
    "           3     0.9610    0.9752    0.9681       202\n",
    "           4     0.9350    0.9947    0.9639       188\n",
    "           5     0.9333    0.9785    0.9554       186\n",
    "           6     0.8750    1.0000    0.9333        28\n",
    "           7     0.9487    0.9893    0.9686       187\n",
    "           8     1.0000    0.9681    0.9838       188\n",
    "           9     0.9575    0.9531    0.9553       213\n",
    "          10     0.9569    0.9950    0.9756       201\n",
    "          11     0.9388    0.9154    0.9270       201\n",
    "          12     0.9660    0.9900    0.9779       201\n",
    "          13     0.9486    0.9442    0.9464       215\n",
    "          14     0.9660    0.9803    0.9731       203\n",
    "          15     0.9594    0.9000    0.9287       210\n",
    "          16     0.9267    0.9620    0.9440       184\n",
    "          17     0.9493    0.9493    0.9493       217\n",
    "          18     0.9794    0.9500    0.9645       200\n",
    "          19     0.9305    0.9305    0.9305       187\n",
    "          20     0.9655    0.9751    0.9703       201\n",
    "          21     0.9737    0.8768    0.9227       211\n",
    "          22     0.9081    0.9231    0.9155       182\n",
    "          23     0.9875    0.9851    0.9863       402\n",
    "          24     0.9573    0.9573    0.9573       211\n",
    "          25     0.9594    0.9643    0.9618       196\n",
    "          26     0.8599    0.9271    0.8922       192\n",
    "          27     0.9766    0.9766    0.9766       214\n",
    "          28     0.9458    0.9100    0.9275       211\n",
    "          29     0.9630    0.9945    0.9785       183\n",
    "          30     0.9420    0.9653    0.9535       202\n",
    "          31     0.9019    0.9415    0.9212       205\n",
    "          32     0.9728    0.8995    0.9347       199\n",
    "          33     0.9209    0.8956    0.9081       182\n",
    "          34     0.9876    0.8368    0.9060       190\n",
    "          35     0.9115    0.9459    0.9284       185\n",
    "          36     0.9844    0.9403    0.9618       201\n",
    "          37     0.9672    0.9415    0.9542       188\n",
    "\n",
    "    accuracy                         0.9528      7603\n",
    "   macro avg     0.9503    0.9528    0.9509      7603\n",
    "weighted avg     0.9535    0.9528    0.9526      7603\n",
    "\n",
    "ResNet 18\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       190\n",
    "           1     0.9808    0.9903    0.9855       206\n",
    "           2     0.9395    0.9665    0.9528       209\n",
    "           3     0.9415    0.9620    0.9516       184\n",
    "           4     0.9064    0.9946    0.9485       185\n",
    "           5     0.9227    0.9275    0.9251       193\n",
    "           6     0.9688    1.0000    0.9841        31\n",
    "           7     0.9662    0.9852    0.9756       203\n",
    "           8     1.0000    0.9708    0.9852       171\n",
    "           9     0.9529    0.9381    0.9455       194\n",
    "          10     0.9020    0.9840    0.9412       187\n",
    "          11     0.9310    0.9450    0.9380       200\n",
    "          12     0.9670    0.9903    0.9785       207\n",
    "          13     0.9167    0.9444    0.9303       198\n",
    "          14     0.9363    0.9695    0.9526       197\n",
    "          15     0.8768    0.9069    0.8916       204\n",
    "          16     0.8622    0.9602    0.9086       176\n",
    "          17     0.9596    0.9360    0.9476       203\n",
    "          18     0.9375    0.9326    0.9351       193\n",
    "          19     0.8722    0.9340    0.9021       212\n",
    "          20     0.9431    0.9522    0.9476       209\n",
    "          21     0.9198    0.9198    0.9198       212\n",
    "          22     0.9072    0.8381    0.8713       210\n",
    "          23     0.9764    0.9764    0.9764       424\n",
    "          24     0.9000    0.8680    0.8837       197\n",
    "          25     0.9000    0.9524    0.9254       189\n",
    "          26     0.9163    0.9336    0.9249       211\n",
    "          27     0.9737    0.9439    0.9585       196\n",
    "          28     0.9381    0.9100    0.9239       200\n",
    "          29     0.9635    0.9860    0.9746       214\n",
    "          30     0.9113    0.9487    0.9296       195\n",
    "          31     0.8856    0.8203    0.8517       217\n",
    "          32     0.9405    0.8614    0.8992       202\n",
    "          33     0.9231    0.8528    0.8865       197\n",
    "          34     0.9293    0.8104    0.8658       211\n",
    "          35     0.8923    0.9158    0.9039       190\n",
    "          36     0.9724    0.8462    0.9049       208\n",
    "          37     0.9438    0.9438    0.9438       178\n",
    "\n",
    "    accuracy                         0.9334      7603\n",
    "   macro avg     0.9336    0.9347    0.9334      7603\n",
    "weighted avg     0.9340    0.9334    0.9331      7603\n",
    "\n",
    "ResNet 50\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       199\n",
    "           1     0.9614    0.9950    0.9779       200\n",
    "           2     0.9621    0.9667    0.9644       210\n",
    "           3     0.9852    0.9709    0.9780       206\n",
    "           4     0.9596    0.9948    0.9769       191\n",
    "           5     0.9417    0.9798    0.9604       198\n",
    "           6     0.9677    0.9677    0.9677        31\n",
    "           7     0.9688    0.9841    0.9764       189\n",
    "           8     0.9942    0.9942    0.9942       173\n",
    "           9     0.9617    0.9617    0.9617       183\n",
    "          10     0.9441    0.9769    0.9602       173\n",
    "          11     0.9571    0.9526    0.9549       211\n",
    "          12     0.9757    0.9805    0.9781       205\n",
    "          13     0.9415    0.9507    0.9461       203\n",
    "          14     0.9725    0.9620    0.9672       184\n",
    "          15     0.9476    0.9050    0.9258       200\n",
    "          16     0.9343    0.9476    0.9409       210\n",
    "          17     0.9583    0.9436    0.9509       195\n",
    "          18     0.9813    0.9502    0.9655       221\n",
    "          19     0.9091    0.9633    0.9354       218\n",
    "          20     0.9403    0.9895    0.9643       191\n",
    "          21     0.9521    0.9353    0.9436       170\n",
    "          22     0.8973    0.9617    0.9284       209\n",
    "          23     0.9951    0.9855    0.9903       413\n",
    "          24     0.8458    0.9659    0.9019       176\n",
    "          25     0.9848    0.9512    0.9677       205\n",
    "          26     0.9286    0.9559    0.9420       204\n",
    "          27     0.9894    0.9588    0.9738       194\n",
    "          28     0.9130    0.9251    0.9190       227\n",
    "          29     0.9898    0.9799    0.9848       199\n",
    "          30     0.9482    0.9482    0.9482       193\n",
    "          31     0.8971    0.9242    0.9104       198\n",
    "          32     0.9344    0.9293    0.9319       184\n",
    "          33     0.9573    0.8745    0.9140       231\n",
    "          34     0.9759    0.8394    0.9025       193\n",
    "          35     0.9574    0.8531    0.9023       211\n",
    "          36     0.9471    0.9227    0.9347       194\n",
    "          37     0.9541    0.9858    0.9697       211\n",
    "\n",
    "    accuracy                         0.9534      7603\n",
    "   macro avg     0.9535    0.9535    0.9530      7603\n",
    "weighted avg     0.9543    0.9534    0.9533      7603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ResNet 50\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       199\n",
    "           1     0.9614    0.9950    0.9779       200\n",
    "           2     0.9621    0.9667    0.9644       210\n",
    "           3     0.9852    0.9709    0.9780       206\n",
    "           4     0.9596    0.9948    0.9769       191\n",
    "           5     0.9417    0.9798    0.9604       198\n",
    "           6     0.9677    0.9677    0.9677        31\n",
    "           7     0.9688    0.9841    0.9764       189\n",
    "           8     0.9942    0.9942    0.9942       173\n",
    "           9     0.9617    0.9617    0.9617       183\n",
    "          10     0.9441    0.9769    0.9602       173\n",
    "          11     0.9571    0.9526    0.9549       211\n",
    "          12     0.9757    0.9805    0.9781       205\n",
    "          13     0.9415    0.9507    0.9461       203\n",
    "          14     0.9725    0.9620    0.9672       184\n",
    "          15     0.9476    0.9050    0.9258       200\n",
    "          16     0.9343    0.9476    0.9409       210\n",
    "          17     0.9583    0.9436    0.9509       195\n",
    "          18     0.9813    0.9502    0.9655       221\n",
    "          19     0.9091    0.9633    0.9354       218\n",
    "          20     0.9403    0.9895    0.9643       191\n",
    "          21     0.9521    0.9353    0.9436       170\n",
    "          22     0.8973    0.9617    0.9284       209\n",
    "          23     0.9951    0.9855    0.9903       413\n",
    "          24     0.8458    0.9659    0.9019       176\n",
    "          25     0.9848    0.9512    0.9677       205\n",
    "          26     0.9286    0.9559    0.9420       204\n",
    "          27     0.9894    0.9588    0.9738       194\n",
    "          28     0.9130    0.9251    0.9190       227\n",
    "          29     0.9898    0.9799    0.9848       199\n",
    "          30     0.9482    0.9482    0.9482       193\n",
    "          31     0.8971    0.9242    0.9104       198\n",
    "          32     0.9344    0.9293    0.9319       184\n",
    "          33     0.9573    0.8745    0.9140       231\n",
    "          34     0.9759    0.8394    0.9025       193\n",
    "          35     0.9574    0.8531    0.9023       211\n",
    "          36     0.9471    0.9227    0.9347       194\n",
    "          37     0.9541    0.9858    0.9697       211\n",
    "\n",
    "    accuracy                         0.9534      7603\n",
    "   macro avg     0.9535    0.9535    0.9530      7603\n",
    "weighted avg     0.9543    0.9534    0.9533      7603\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RESNET 101 \n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       227\n",
    "           1     0.9471    1.0000    0.9729       215\n",
    "           2     0.9453    0.9744    0.9596       195\n",
    "           3     0.9610    0.9752    0.9681       202\n",
    "           4     0.9350    0.9947    0.9639       188\n",
    "           5     0.9333    0.9785    0.9554       186\n",
    "           6     0.8750    1.0000    0.9333        28\n",
    "           7     0.9487    0.9893    0.9686       187\n",
    "           8     1.0000    0.9681    0.9838       188\n",
    "           9     0.9575    0.9531    0.9553       213\n",
    "          10     0.9569    0.9950    0.9756       201\n",
    "          11     0.9388    0.9154    0.9270       201\n",
    "          12     0.9660    0.9900    0.9779       201\n",
    "          13     0.9486    0.9442    0.9464       215\n",
    "          14     0.9660    0.9803    0.9731       203\n",
    "          15     0.9594    0.9000    0.9287       210\n",
    "          16     0.9267    0.9620    0.9440       184\n",
    "          17     0.9493    0.9493    0.9493       217\n",
    "          18     0.9794    0.9500    0.9645       200\n",
    "          19     0.9305    0.9305    0.9305       187\n",
    "          20     0.9655    0.9751    0.9703       201\n",
    "          21     0.9737    0.8768    0.9227       211\n",
    "          22     0.9081    0.9231    0.9155       182\n",
    "          23     0.9875    0.9851    0.9863       402\n",
    "          24     0.9573    0.9573    0.9573       211\n",
    "          25     0.9594    0.9643    0.9618       196\n",
    "          26     0.8599    0.9271    0.8922       192\n",
    "          27     0.9766    0.9766    0.9766       214\n",
    "          28     0.9458    0.9100    0.9275       211\n",
    "          29     0.9630    0.9945    0.9785       183\n",
    "          30     0.9420    0.9653    0.9535       202\n",
    "          31     0.9019    0.9415    0.9212       205\n",
    "          32     0.9728    0.8995    0.9347       199\n",
    "          33     0.9209    0.8956    0.9081       182\n",
    "          34     0.9876    0.8368    0.9060       190\n",
    "          35     0.9115    0.9459    0.9284       185\n",
    "          36     0.9844    0.9403    0.9618       201\n",
    "          37     0.9672    0.9415    0.9542       188\n",
    "\n",
    "    accuracy                         0.9528      7603\n",
    "   macro avg     0.9503    0.9528    0.9509      7603\n",
    "weighted avg     0.9535    0.9528    0.9526      7603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     1.0000    1.0000    1.0000       187\n",
    "           1     0.9292    0.9949    0.9610       198\n",
    "           2     0.9100    0.9630    0.9357       189\n",
    "           3     0.9261    0.9641    0.9447       195\n",
    "           4     0.8966    1.0000    0.9455       182\n",
    "           5     0.8607    0.9153    0.8872       189\n",
    "           6     0.9667    0.8286    0.8923        35\n",
    "           7     0.9447    0.9895    0.9666       190\n",
    "           8     0.9626    0.9945    0.9783       181\n",
    "           9     0.9067    0.9444    0.9252       216\n",
    "          10     0.8813    0.9847    0.9301       196\n",
    "          11     0.8981    0.8645    0.8810       214\n",
    "          12     0.9261    0.9495    0.9377       198\n",
    "          13     0.9146    0.9239    0.9192       197\n",
    "          14     0.7715    0.9493    0.8512       217\n",
    "          15     0.8233    0.9124    0.8655       194\n",
    "          16     0.8894    0.9571    0.9220       210\n",
    "          17     0.8971    0.9015    0.8993       203\n",
    "          18     0.9459    0.8794    0.9115       199\n",
    "          19     0.8514    0.9000    0.8750       210\n",
    "          20     0.8868    0.9400    0.9126       200\n",
    "          21     0.9162    0.8706    0.8929       201\n",
    "          22     0.8592    0.8971    0.8777       204\n",
    "          23     0.9759    0.9479    0.9617       384\n",
    "          24     0.7764    0.9183    0.8414       208\n",
    "          25     0.9130    0.9175    0.9153       206\n",
    "          26     0.8814    0.8465    0.8636       202\n",
    "          27     0.9392    0.9770    0.9577       174\n",
    "          28     0.9365    0.8510    0.8917       208\n",
    "          29     0.9013    0.9901    0.9437       203\n",
    "          30     0.9032    0.7534    0.8215       223\n",
    "          31     0.9200    0.7778    0.8429       207\n",
    "          32     0.9034    0.8281    0.8641       192\n",
    "          33     0.8693    0.7727    0.8182       198\n",
    "          34     0.9444    0.8252    0.8808       206\n",
    "          35     0.9329    0.6715    0.7809       207\n",
    "          36     0.9091    0.8621    0.8850       174\n",
    "          37     0.9598    0.8107    0.8789       206\n",
    "\n",
    "    accuracy                         0.9029      7603\n",
    "   macro avg     0.9061    0.9019    0.9016      7603\n",
    "weighted avg     0.9056    0.9029    0.9019      7603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genrate descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T01:42:45.490830Z",
     "iopub.status.busy": "2024-12-18T01:42:45.489949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.45.1\n",
    "%pip install gradio==4.44.0\n",
    "%pip install accelerate==0.34.2\n",
    "%pip install bitsandbytes==0.44.0\n",
    "%pip install tensorflow-cpu==2.17.0\n",
    "%pip install einops==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T01:42:13.865154Z",
     "iopub.status.busy": "2024-12-18T01:42:13.864757Z",
     "iopub.status.idle": "2024-12-18T01:42:13.913169Z",
     "shell.execute_reply": "2024-12-18T01:42:13.911701Z",
     "shell.execute_reply.started": "2024-12-18T01:42:13.865122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27/3481050359.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMllamaForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "# Set memory management for PyTorch\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'  # or adjust size as needed\n",
    "\n",
    "# Model selection menu in terminal\n",
    "print(\"Select a model to load:\")\n",
    "print(\"1. Llama-3.2-11B-Vision-Instruct-bnb-4bit\")\n",
    "print(\"2. Molmo-7B-D-bnb-4bit\")\n",
    "model_choice = input(\"Enter the number of the model you want to use: \")\n",
    "\n",
    "if model_choice == \"1\":\n",
    "\n",
    "    #model_id = \"unsloth/Llama-3.2-7B-Vision-Instruct-bnb-4bit\"  # Use a smaller model\n",
    "\n",
    "    model_id = \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\"\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit_fp32_cpu_offload=True,  # Enable CPU offloading\n",
    "    device_map=\"auto\",  # Automatically assign parts of the model to CPU/GPU\n",
    "     ) \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "elif model_choice == \"2\":\n",
    "    model_id = \"cyan2k/molmo-7B-D-bnb-4bit\"\n",
    "    # Remove device_map and torch_dtype to use CPU\n",
    "    arguments = {\"trust_remote_code\": True}  \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, **arguments)\n",
    "    processor = AutoProcessor.from_pretrained(model_id, **arguments)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid model choice. Please enter 1 or 2.\")\n",
    "\n",
    "# Visual theme\n",
    "visual_theme = gr.themes.Default()  # Default, Soft or Monochrome\n",
    "\n",
    "# Constants\n",
    "MAX_OUTPUT_TOKENS = 2048\n",
    "MAX_IMAGE_SIZE = (1120, 1120)\n",
    "\n",
    "# Function to process the image and generate a description\n",
    "def describe_image(image, user_prompt, temperature, top_k, top_p, max_tokens, history):\n",
    "    # Resize image if necessary\n",
    "    image = image.resize(MAX_IMAGE_SIZE)\n",
    "\n",
    "    # Initialize cleaned_output variable\n",
    "    cleaned_output = \"\"\n",
    "\n",
    "    # Prepare prompt with user input based on selected model\n",
    "    if model_choice == \"1\":  # Llama Model\n",
    "        prompt = f\"<|image|><|begin_of_text|>{user_prompt} Answer:\"\n",
    "        # Preprocess the image and prompt\n",
    "        inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate output with model\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=min(max_tokens, MAX_OUTPUT_TOKENS),\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "        # Decode the raw output\n",
    "        raw_output = processor.decode(output[0])\n",
    "\n",
    "        # Clean up the output to remove system tokens\n",
    "        cleaned_output = raw_output.replace(\"<|image|><|begin_of_text|>\", \"\").strip().replace(\" Answer:\", \"\")\n",
    "\n",
    "    elif model_choice == \"2\":  # Molmo Model\n",
    "        # Prepare inputs for Molmo model\n",
    "        inputs = processor.process(images=[image], text=user_prompt)\n",
    "        inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate output with model, applying the parameters for temperature, top_k, top_p, and max_tokens\n",
    "        output = model.generate_from_batch(\n",
    "            inputs,\n",
    "            GenerationConfig(\n",
    "                max_new_tokens=min(max_tokens, MAX_OUTPUT_TOKENS),\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                stop_strings=\"<|endoftext|>\",\n",
    "                do_sample=True\n",
    "            ),\n",
    "            tokenizer=processor.tokenizer,\n",
    "        )\n",
    "\n",
    "        # Extract generated tokens and decode them to text\n",
    "        generated_tokens = output[0, inputs[\"input_ids\"].size(1):]\n",
    "        cleaned_output = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the prompt is not repeated in the output\n",
    "    if cleaned_output.startswith(user_prompt):\n",
    "        cleaned_output = cleaned_output[len(user_prompt):].strip()\n",
    "\n",
    "    # Append the new conversation to the history\n",
    "    history.append((user_prompt, cleaned_output))\n",
    "\n",
    "    return history\n",
    "\n",
    "# Function to clear the chat history\n",
    "def clear_chat():\n",
    "    return []\n",
    "\n",
    "# Gradio Interface\n",
    "def gradio_interface():\n",
    "    with gr.Blocks(visual_theme) as demo:\n",
    "        gr.HTML(\n",
    "        \"\"\"\n",
    "    <h1 style='text-align: center'>\n",
    "    Clean-UI\n",
    "    </h1>\n",
    "    \"\"\")\n",
    "        with gr.Row():\n",
    "            # Left column with image and parameter inputs\n",
    "            with gr.Column(scale=1):\n",
    "                image_input = gr.Image(\n",
    "                    label=\"Image\",\n",
    "                    type=\"pil\",\n",
    "                    image_mode=\"RGB\",\n",
    "                    height=512,  # Set the height\n",
    "                    width=512   # Set the width\n",
    "                )\n",
    "\n",
    "                # Parameter sliders\n",
    "                temperature = gr.Slider(\n",
    "                    label=\"Temperature\", minimum=0.1, maximum=2.0, value=0.6, step=0.1, interactive=True)\n",
    "                top_k = gr.Slider(\n",
    "                    label=\"Top-k\", minimum=1, maximum=100, value=50, step=1, interactive=True)\n",
    "                top_p = gr.Slider(\n",
    "                    label=\"Top-p\", minimum=0.1, maximum=1.0, value=0.9, step=0.1, interactive=True)\n",
    "                max_tokens = gr.Slider(\n",
    "                    label=\"Max Tokens\", minimum=50, maximum=MAX_OUTPUT_TOKENS, value=100, step=50, interactive=True)\n",
    "\n",
    "            # Right column with the chat interface\n",
    "            with gr.Column(scale=2):\n",
    "                chat_history = gr.Chatbot(label=\"Chat\", height=512)\n",
    "\n",
    "                # User input box for prompt\n",
    "                user_prompt = gr.Textbox(\n",
    "                    show_label=False,\n",
    "                    container=False,\n",
    "                    placeholder=\"Enter your prompt\",\n",
    "                    lines=2\n",
    "                )\n",
    "\n",
    "                # Generate and Clear buttons\n",
    "                with gr.Row():\n",
    "                    generate_button = gr.Button(\"Generate\")\n",
    "                    clear_button = gr.Button(\"Clear\")\n",
    "\n",
    "                # Define the action for the generate button\n",
    "                generate_button.click(\n",
    "                    fn=describe_image,\n",
    "                    inputs=[image_input, user_prompt, temperature, top_k, top_p, max_tokens, chat_history],\n",
    "                    outputs=[chat_history]\n",
    "                )\n",
    "\n",
    "                # Define the action for the clear button\n",
    "                clear_button.click(\n",
    "                    fn=clear_chat,\n",
    "                    inputs=[],\n",
    "                    outputs=[chat_history]\n",
    "                )\n",
    "\n",
    "    return demo\n",
    "\n",
    "# Launch the interface\n",
    "demo = gradio_interface()\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T06:19:34.657489Z",
     "iopub.status.busy": "2024-12-06T06:19:34.657075Z",
     "iopub.status.idle": "2024-12-06T06:19:35.389227Z",
     "shell.execute_reply": "2024-12-06T06:19:35.387446Z",
     "shell.execute_reply.started": "2024-12-06T06:19:34.657455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/476 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 1, 52, 52] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27/3661024711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27/1580095030.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 460\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 1, 52, 52] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "net = resnet101(num_classes=38)\n",
    "net.to(device)\n",
    "\n",
    "loss_function = CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=3e-5)\n",
    "epochs = 30\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "    ####################### Training #######################\n",
    "    net.train()\n",
    "    acc_num = torch.zeros(1).to(device)\n",
    "    sample_num = 0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, file=sys.stdout, ncols=100)\n",
    "    for step, (images, labels) in enumerate(train_bar):\n",
    "        sample_num += images.size(0)\n",
    "        images, labels = images.float().to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        pred_class = outputs.argmax(dim=1)\n",
    "        acc_num += (pred_class == labels).sum()\n",
    "\n",
    "        train_acc = acc_num.item() / sample_num\n",
    "        train_bar.desc = f\"train epoch[{epoch+1}/{epochs}] loss:{loss:.3f} acc:{train_acc:.3f}\"\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    ####################### Validation #######################\n",
    "    net.eval()\n",
    "    acc_num = 0.0\n",
    "    val_num = len(test_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in test_loader:\n",
    "            val_images, val_labels = val_images.float().to(device), val_labels.to(device)\n",
    "            outputs = net(val_images)\n",
    "            pred_class = outputs.argmax(dim=1)\n",
    "            acc_num += (pred_class == val_labels).sum().item()\n",
    "\n",
    "    val_accurate = acc_num / val_num\n",
    "    print(f\"[epoch {epoch+1}] train_loss: {avg_loss:.3f} train_acc: {train_acc:.3f} val_accuracy: {val_accurate:.3f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        torch.save(net.state_dict(), \"best_resnet101_model.pth\")\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 899128,
     "sourceId": 10404507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30461,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
